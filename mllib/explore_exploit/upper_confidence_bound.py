import numpy as np
from matplotlib import pyplot as plt


class BanditArm:
    def __init__(self, generator):
        self.generator = generator
        self.n_explorations = 0
        self.avg = None

    def pull(self):
        return self.generator()

    def update(self, x):
        self.n_explorations += 1
        if self.avg is not None:
            self.avg = self.avg + (x - self.avg) / self.n_explorations
        else:
            self.avg = x

    def pull_update(self):
        pulled = self.pull()
        self.update(pulled)
        return pulled


def bernoulli(p):
    return 1 if np.random.random() < p else 0


def upper_confidence_bound_explore_exploit(bandits_factory, n_trials, print_results=False):
    # as upper bound we choose P(X_mu - EX >= t) <= np.sqrt(2 * np.log(N) / n_j)
    def ucb1(estimate, n, n_j):
        return estimate + np.sqrt(2 * np.log(n) / n_j)

    rewards = []
    total_n_explorations = 0

    bandits = bandits_factory()
    for bandit in bandits:
        rewards.append(bandit.pull_update())
        total_n_explorations += 1

    n_trials = n_trials - total_n_explorations

    bandit_avgs = np.zeros((n_trials, len(bandits)))
    for i in range(n_trials):
        # take best bandit
        bandit_id = np.argmax(np.array(
            [ucb1(b.avg, total_n_explorations, b.n_explorations) for b in bandits if b.avg is not None]
        ))
        bandit = bandits[bandit_id]
        rewards.append(bandit.pull_update())
        bandit_avgs[i, :] = np.array([b.avg for b in bandits])
        total_n_explorations += 1

    if print_results:
        print(f'overall win rate: {np.mean(rewards)}')
        for i, bandit in enumerate(bandits):
            print(f'bandit {i} explorations: {bandit.n_explorations}, final avg estimate: {bandit.avg}')

    return rewards, bandit_avgs


def plot_experiment_results(experiments_rewards, experiments_bandit_avgs, benchmark):
    plt.figure(figsize=(16, 16))
    for i in range(16):
        rewards = experiments_rewards[i]
        bandits_avgs = experiments_bandit_avgs[i]
        n = len(rewards)
        cumulative_rewards = np.cumsum(rewards)
        win_rates = cumulative_rewards / (np.arange(n) + 1)

        plt.subplot(4, 4, i + 1)
        plt.plot(win_rates)
        for bi in range(bandits_avgs.shape[1]):
            plt.plot(bandits_avgs[:, bi], label=f'bandit {bi}')
        plt.plot(np.ones(n) * benchmark)
        plt.title(f'experiment {i}')
        plt.legend()
    plt.show()


def bandits_generator():
    bandits = []
    bandits.append(BanditArm(lambda: bernoulli(0.7)))
    bandits.append(BanditArm(lambda: bernoulli(0.6)))
    bandits.append(BanditArm(lambda: bernoulli(0.8)))
    bandits.append(BanditArm(lambda: bernoulli(0.9)))
    return bandits


if __name__ == '__main__':
    n_experiments = 16
    n_bandit_pulls = 10000

    reward_avgs = []
    experiments_bandit_avgs = []

    for i in range(n_experiments):
        print(f'*** experiment {i} ***')
        rewards, bandit_avgs = upper_confidence_bound_explore_exploit(
            bandits_factory=bandits_generator,
            n_trials=n_bandit_pulls,
            print_results=True)
        reward_avgs.append(rewards)
        experiments_bandit_avgs.append(bandit_avgs)

    print(f'\n*** SUMMARY AFTER {n_experiments} ***')
    print(f'average overall experiment win rate: {np.mean(reward_avgs)}')

    plot_experiment_results(reward_avgs, experiments_bandit_avgs, 0.9)
