import numpy as np
from matplotlib import pyplot as plt


class BanditArm:
    def __init__(self, generator):
        self.generator = generator
        self.n_explorations = 0
        self.avg = None

    def pull(self):
        return self.generator()

    def update(self, x):
        self.n_explorations += 1
        if self.avg is not None:
            self.avg = self.avg + (x - self.avg) / self.n_explorations
        else:
            self.avg = x

    def pull_update(self):
        pulled = self.pull()
        self.update(pulled)
        return pulled


def bernoulli(p):
    return 1 if np.random.random() < p else 0


def epsilon_greedy_explore_exploit(bandits_factory, true_best_bandit_id, n_trials, epsilon=0.1, max_explorations=None, initial_explorations_ber_bandit=1, print_results=False):
    # this implementation of epsilon-greedy algorithm use constant epsilon, where us a good approach is to use some
    # epsilon decreasing over time by some strategy, eg 1/n or exponential

    bandits = bandits_factory()
    assert n_trials >= len(bandits) * initial_explorations_ber_bandit
    rewards = []
    n_exporations = 0
    n_exploitations = 0
    true_best_bandit_explotations = 0

    if max_explorations is None:
        max_explorations = n_trials

    for i in range(initial_explorations_ber_bandit):
        for b_i, bandit in enumerate(bandits):
            rewards.append(bandit.pull_update())
            n_exporations += 1
            if b_i == true_best_bandit_id:
                true_best_bandit_explotations += 1
    n_trials = n_trials - n_exporations

    ultimate_best_bandit_id = None
    for i in range(n_trials):
        if ultimate_best_bandit_id is None:
            if bernoulli(epsilon) == 1 or n_exporations == 0:
                # choose bandit randomly
                bandit_id = np.random.choice(len(bandits))
                n_exporations += 1
            else:
                # take best bandit
                bandit_id = np.argmax(np.array([b.avg for b in bandits if b.avg is not None]))
                n_exploitations += 1

            bandit = bandits[bandit_id]
            rewards.append(bandit.pull_update())
            if bandit_id == true_best_bandit_id:
                true_best_bandit_explotations += 1

            if i > max_explorations - 2:
                ultimate_best_bandit_id = np.argmax(np.array([b.avg for b in bandits if b.avg is not None]))
        else:
            rewards.append(bandits[ultimate_best_bandit_id].pull_update())
            n_exploitations += 1
            if ultimate_best_bandit_id == true_best_bandit_id:
                true_best_bandit_explotations += 1

    if print_results:
        print(f'epsilon-greedy win rate: {np.mean(rewards)}')
        print(f'number of explorations: {n_exporations}')
        print(f'number of exploitations: {n_exploitations}')
        print(f'number of true best bandit exploitations: {true_best_bandit_explotations}')

    return rewards, n_exporations, n_exploitations, true_best_bandit_explotations


def plot_experiment_results(experiments_rewards, benchmark):
    plt.figure(figsize=(16, 16))
    for i in range(16):
        rewards = experiments_rewards[i]
        n = len(rewards)
        cumulative_rewards = np.cumsum(rewards)
        win_rates = cumulative_rewards / (np.arange(n) + 1)

        plt.subplot(4, 4, i + 1)
        plt.plot(win_rates)
        plt.plot(np.ones(n) * benchmark)
        plt.title(f'experiment {i}')
    plt.show()


def bandits_generator():
    bandits = []
    bandits.append(BanditArm(lambda: bernoulli(0.7)))
    bandits.append(BanditArm(lambda: bernoulli(0.6)))
    bandits.append(BanditArm(lambda: bernoulli(0.8)))
    bandits.append(BanditArm(lambda: bernoulli(0.9)))
    return bandits


if __name__ == '__main__':
    n_experiments = 100
    n_bandit_pulls = 10000

    reward_avgs = []
    exporations = []
    explotations = []
    true_best_explotations = []

    for i in range(n_experiments):
        print(f'*** experiment {i} ***')
        rewards, explor_count, exploit_count, true_best_exploit_count = epsilon_greedy_explore_exploit(
            bandits_factory=bandits_generator,
            true_best_bandit_id=0,
            n_trials=n_bandit_pulls,
            epsilon=0.1,
            max_explorations=n_bandit_pulls,
            initial_explorations_ber_bandit=0,
            print_results=True)
        reward_avgs.append(rewards)
        exporations.append(explor_count)
        explotations.append(exploit_count)
        true_best_explotations.append(true_best_exploit_count)

    print(f'\n*** SUMMARY AFTER {n_experiments} ***')
    print(f'average epsilon-greedy win rate: {np.mean(reward_avgs)}')
    print(f'average number of explorations: {np.mean(exporations)}')
    print(f'average number of exploitations: {np.mean(explotations)}')
    print(f'average number of true best bandit exploitations: {np.mean(true_best_explotations)}')

    plot_experiment_results(reward_avgs, 0.9)
