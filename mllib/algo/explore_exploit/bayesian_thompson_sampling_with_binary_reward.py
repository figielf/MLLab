import numpy as np
from matplotlib import pyplot as plt
from scipy.stats import beta


class BanditArm:
    # Beta prior distribution
    def __init__(self, generator, init_reward=None):
        self.generator = generator
        self.alpha = 1
        self.beta = 1
        self.n_explorations = 0
        self._rewards = init_reward

    def sample(self, n=1):
        return beta.rvs(self.alpha, self.beta, size=n)

    def pull(self):
        return self.generator()

    def update(self, x):
        self.n_explorations += 1
        if self._rewards is not None:
            self._rewards += x
        else:
            self._rewards = x
        self.alpha += x
        self.beta += 1 - x


def bernoulli(p):
    return 1 if np.random.random() < p else 0


def bayesian_thompson_sampling__with_binary_reward_explore_exploit(bandits_factory, n_trials, print_results=False, print_detailed_results=True):
    rewards = []
    bandits = bandits_factory()

    for i in range(n_trials):
        # Thompson sampling
        samples = [b.sample()[0] for b in bandits]

        # choose best bandit
        bandit_id = np.argmax(samples)
        bandit = bandits[bandit_id]
        explore_value = bandit.pull()
        bandit.update(explore_value)
        rewards.append(explore_value)
        if print_detailed_results:
            print(f'trial {i} - rewarded {explore_value} from bandit {bandit_id} as {bandit._rewards}/{bandit.n_explorations}, its avg value is {bandit._rewards / bandit.n_explorations}')
            print(f'\tsamples from beta: {samples}')

    if print_results:
        print(f'overall win rate: {np.mean(rewards)}')
        for i, bandit in enumerate(bandits):
            print(f'bandit {i} explorations: {bandit.n_explorations}/{n_trials}, final avg estimate: {bandit._rewards / bandit.n_explorations}')

    return rewards, bandits


def plot_experiment_results(experiments_rewards, experiments_bandits, benchmark):
    plt.figure(figsize=(24, 16))
    for i in range(8):
        rewards = experiments_rewards[i]
        bandits = experiments_bandits[i]
        n = len(rewards)
        cumulative_rewards = np.cumsum(rewards)
        win_rates = cumulative_rewards / (np.arange(n) + 1)

        plt.subplot(4, 4, i + 1)
        plt.plot(win_rates, label=f'final avg={np.mean(rewards)}')
        plt.plot(np.ones(n) * benchmark, label=f'best true avg={0.9}')
        plt.legend()
        plt.title(f'experiment {i}')

        plt.subplot(4, 4, i + 8 + 1)
        for bi, b in enumerate(bandits):
            x = np.linspace(0, 1, 1000)
            plt.plot(x, beta.pdf(x, b.alpha, b.beta), label=f'bandit {bi}, {b.n_explorations}/{n}')
        plt.title(f'experiment {i} - bets pdf')
        plt.legend()

    plt.show()


def bandits_generator():
    bandits = []
    bandits.append(BanditArm(lambda: bernoulli(0.7), init_reward=0))
    bandits.append(BanditArm(lambda: bernoulli(0.6), init_reward=0))
    bandits.append(BanditArm(lambda: bernoulli(0.8), init_reward=0))
    bandits.append(BanditArm(lambda: bernoulli(0.9), init_reward=0))
    return bandits


if __name__ == '__main__':
    n_experiments = 8
    n_bandit_pulls = 100

    reward_avgs = []
    experiments_bandits = []

    for i in range(n_experiments):
        print(f'*** experiment {i} ***')
        rewards, bandits = bayesian_thompson_sampling__with_binary_reward_explore_exploit(
            bandits_factory=bandits_generator,
            n_trials=n_bandit_pulls,
            print_results=True,
            print_detailed_results=False)
        reward_avgs.append(rewards)
        experiments_bandits.append(bandits)

    print(f'\n*** SUMMARY AFTER {n_experiments} ***')
    print(f'average overall experiment win rate: {np.mean(reward_avgs)}')

    plot_experiment_results(reward_avgs, experiments_bandits, 0.9)
