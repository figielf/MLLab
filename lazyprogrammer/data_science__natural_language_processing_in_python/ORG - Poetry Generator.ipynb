{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ORG - Poetry Generator.ipynb","provenance":[{"file_id":"1oEjHwGfHVdY_Sx8w4BLEuBihORnWJ2w1","timestamp":1650366582331}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"eLh8z_hEUCTY","executionInfo":{"status":"ok","timestamp":1650366989423,"user_tz":-120,"elapsed":371,"user":{"displayName":"Marcin Figlewicz","userId":"11641752571162350955"}}},"source":["import numpy as np\n","import string\n","\n","np.random.seed(1234)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"TR6uHGtBUbFm","executionInfo":{"status":"ok","timestamp":1650366989704,"user_tz":-120,"elapsed":7,"user":{"displayName":"Marcin Figlewicz","userId":"11641752571162350955"}}},"source":["initial = {} # start of a phrase\n","first_order = {} # second word only\n","second_order = {}"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNP9na4yUelU","executionInfo":{"status":"ok","timestamp":1650366989977,"user_tz":-120,"elapsed":5,"user":{"displayName":"Marcin Figlewicz","userId":"11641752571162350955"}}},"source":["def remove_punctuation(s):\n","    return s.translate(str.maketrans('','',string.punctuation))"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jknIxRKzUhwc","executionInfo":{"status":"ok","timestamp":1650366990564,"user_tz":-120,"elapsed":332,"user":{"displayName":"Marcin Figlewicz","userId":"11641752571162350955"}},"outputId":"3c4a550c-768b-4660-c294-079165c993f3"},"source":["!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-04-19 11:16:30--  https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 56286 (55K) [text/plain]\n","Saving to: ‘robert_frost.txt’\n","\n","\rrobert_frost.txt      0%[                    ]       0  --.-KB/s               \rrobert_frost.txt    100%[===================>]  54.97K  --.-KB/s    in 0.01s   \n","\n","2022-04-19 11:16:30 (5.23 MB/s) - ‘robert_frost.txt’ saved [56286/56286]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"S0mbp9FPUk4o","executionInfo":{"status":"ok","timestamp":1650366994462,"user_tz":-120,"elapsed":287,"user":{"displayName":"Marcin Figlewicz","userId":"11641752571162350955"}}},"source":["def add2dict(d, k, v):\n","  if k not in d:\n","    d[k] = []\n","  d[k].append(v)\n","\n","# [cat, cat, dog, dog, dog, dog, dog, mouse, ...]"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"FoBDJVw3UrbE"},"source":["for line in open('robert_frost.txt'):\n","  tokens = remove_punctuation(line.rstrip().lower()).split()\n","\n","  T = len(tokens)\n","  for i in range(T):\n","    t = tokens[i]\n","    if i == 0:\n","      # measure the distribution of the first word\n","      initial[t] = initial.get(t, 0.) + 1\n","    else:\n","      t_1 = tokens[i-1]\n","      if i == T - 1:\n","        # measure probability of ending the line\n","        add2dict(second_order, (t_1, t), 'END')\n","      if i == 1:\n","        # measure distribution of second word\n","        # given only first word\n","        add2dict(first_order, t_1, t)\n","      else:\n","        t_2 = tokens[i-2]\n","        add2dict(second_order, (t_2, t_1), t)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dq98tbyWU7J5"},"source":["# normalize the distributions\n","initial_total = sum(initial.values())\n","for t, c in initial.items():\n","    initial[t] = c / initial_total"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gWr05mfxVBqH"},"source":["# convert [cat, cat, cat, dog, dog, dog, dog, mouse, ...]\n","# into {cat: 0.5, dog: 0.4, mouse: 0.1}\n","\n","def list2pdict(ts):\n","  # turn each list of possibilities into a dictionary of probabilities\n","  d = {}\n","  n = len(ts)\n","  for t in ts:\n","    d[t] = d.get(t, 0.) + 1\n","  for t, c in d.items():\n","    d[t] = c / n\n","  return d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rR-QfKjZVMTC"},"source":["for t_1, ts in first_order.items():\n","  # replace list with dictionary of probabilities\n","  first_order[t_1] = list2pdict(ts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZC4AFs8VPZS"},"source":["for k, ts in second_order.items():\n","  second_order[k] = list2pdict(ts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTSW8KwMVX8g"},"source":["def sample_word(d):\n","  # print \"d:\", d\n","  p0 = np.random.random()\n","  # print \"p0:\", p0\n","  cumulative = 0\n","  for t, p in d.items():\n","    cumulative += p\n","    if p0 < cumulative:\n","      return t\n","  assert(False) # should never get here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rgLEb5NoVeUW"},"source":["def generate():\n","  for i in range(4): # generate 4 lines\n","    sentence = []\n","\n","    # initial word\n","    w0 = sample_word(initial)\n","    sentence.append(w0)\n","\n","    # sample second word\n","    w1 = sample_word(first_order[w0])\n","    sentence.append(w1)\n","\n","    # second-order transitions until END\n","    while True:\n","      w2 = sample_word(second_order[(w0, w1)])\n","      if w2 == 'END':\n","        break\n","      sentence.append(w2)\n","      w0 = w1\n","      w1 = w2\n","    print(' '.join(sentence))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HnkxngVlVn7q","executionInfo":{"status":"ok","timestamp":1631154882021,"user_tz":240,"elapsed":99,"user":{"displayName":"Lazy Programmer","photoUrl":"","userId":"14255399010036105472"}},"outputId":"1b4f58c3-f36e-41c3-a16d-da71e92b3a51"},"source":["generate()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dyou know a person so related to herself\n","you take the polish off the ground\n","no i dont follow you\n","in leaves no step had trodden black\n"]}]},{"cell_type":"code","metadata":{"id":"Rqzv5F79Vps7"},"source":["# Exercise:\n","#\n","# Determine the vocabulary size (V)\n","# We know that pi has shape V, A1 has shape V x V, and A2 has shape V x V x V\n","#\n","# In comparison, how many values are stored in our dictionaries?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQ3nEBnf69F7"},"source":["# Exercise 2:\n","# We can skip the step where we accumulate all the possible next words in a list\n","# E.g. [cat, cat, dog, dog, dog, ...]\n","#\n","# Instead, like we do with the initial state distribution, create the dictionary\n","# of counts directly as you loop through the data.\n","#\n","# You'll no longer need list2pdict()"],"execution_count":null,"outputs":[]}]}