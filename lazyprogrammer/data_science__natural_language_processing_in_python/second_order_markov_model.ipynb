{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"second_order_markov_model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO1KJl7BbwAkcQuI+SbgCvz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import string\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, f1_score\n","\n","pd.set_option('display.max_colwidth', -1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcDda3RKD39M","executionInfo":{"status":"ok","timestamp":1650875961199,"user_tz":-120,"elapsed":1403,"user":{"displayName":"Marcin Figlewicz","userId":"11641752571162350955"}},"outputId":"6bc1205f-d966-46b1-b2f8-c356f05a7b59"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  \n"]}]},{"cell_type":"code","source":["np.random.random()"],"metadata":{"id":"524wPMhI3-1h","executionInfo":{"status":"ok","timestamp":1650875980127,"user_tz":-120,"elapsed":426,"user":{"displayName":"Marcin Figlewicz","userId":"11641752571162350955"}},"outputId":"10a25dfc-e78e-4518-c8aa-acf9aea6fbac","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7098987032239765"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# Load data"],"metadata":{"id":"_q8cHUnIDdEM"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)\n","\n","poem_lines = []\n","with open('/content/gdrive/MyDrive/Colab Notebooks/lazyprogrammer/data/robert_frost.txt', 'r') as file:\n","  for line in file:\n","    txt = line.rstrip().lower()\n","    if txt:\n","      txt = txt.translate(str.maketrans('', '', string.punctuation))\n","      poem_lines.append(txt)\n","\n","poem_lines = np.array(poem_lines)\n","print(poem_lines[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ikDxNItmDbkT","executionInfo":{"status":"ok","timestamp":1650375001048,"user_tz":-120,"elapsed":4633,"user":{"displayName":"Marcin Figlewicz","userId":"11641752571162350955"}},"outputId":"07b949e8-cfee-4798-f8b1-2b558d9345f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n","['two roads diverged in a yellow wood' 'and sorry i could not travel both'\n"," 'and be one traveler long i stood'\n"," 'and looked down one as far as i could'\n"," 'to where it bent in the undergrowth']\n"]}]},{"cell_type":"code","source":["X_train = poem_lines"],"metadata":{"id":"7U_zUOWQynFw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Utils"],"metadata":{"id":"4F-VsSS8yd2_"}},{"cell_type":"code","source":["END_TOKEN = '<END>'\n","\n","def build_word2idx(txt_lines, skip_word_filter=None, end_token=None):\n","  word2idx = {'<unknown>': 0}\n","  if end_token is not None:\n","    word2idx = {end_token: 1}\n","  idx = len(word2idx)\n","  for line in txt_lines:\n","    for token in line.split():\n","      if token in word2idx:\n","        continue\n","      elif skip_word_filter is not None and not skip_word_filter(token):\n","        continue\n","      else:\n","        word2idx[token] = idx\n","        idx += 1\n","  return word2idx\n","\n","def tokenize(txt, word2idx, end_token=None):\n","  vector = []\n","  for token in txt.split():\n","    vector.append(word2idx.get(token, 0))\n","  if end_token is not None:\n","    vector.append(word2idx[end_token])\n","  return vector\n","\n","def idx2word(token_idxs, word2idx):\n","  idx2token = {}\n","  for k, v in word2idx.items():\n","    idx2token[v] = k\n","\n","  return [idx2token[idx] for _, idx in enumerate(token_idxs)]\n"],"metadata":{"id":"3NrF3Ke1hCBz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Markov Model class"],"metadata":{"id":"lzG6r8Ng0Pzw"}},{"cell_type":"code","source":["class SparseSecondOrderMarkovModel():\n","  def __init__(self, n_states, seq_end=None):\n","    self.n_states = n_states\n","    self.pi = None\n","    self.A1 = None\n","    self.A2 = None\n","    self.seq_end = seq_end\n","  \n","  def fit(self, x):\n","    assert len(x) > 0\n","\n","    # list all states from x at every sequence position t\n","    x_pi = [] # list(state_0)\n","    x_A1 = {} # dict(state_0, list(state_1))\n","    x_A2 = {} # dict((state_t_2, state_t_1), list(state_t))\n","    for idx, x_cur in enumerate(x):\n","      state_0 = x_cur[0]\n","      x_pi.append(state_0)\n","      x_A1[state_0] = x_A1.get(state_0, [])\n","      if len(x_cur) == 1:\n","        if self.seq_end is not None:\n","          x_A1[state_0].append(self.seq_end)\n","      else:\n","        state_1 = x_cur[1]\n","        x_A1[state_0].append(state_1)\n","        for t in range(2, len(x_cur)):\n","          pair_t_1 = (x_cur[t-2], x_cur[t-1])\n","          x_A2[pair_t_1] = x_A2.get(pair_t_1, [])\n","          x_A2[pair_t_1].append(x_cur[t])\n","\n","    # count the probability of each transition from dictionaries above,\n","    # this will give us sparse matrix (implemented as dictionary) of transition states\n","    self.pi = self._calc_discrete_distribution(x_pi) # dict(state_0, state_0 probability at first position)\n","    \n","    self.A1 = {} # dict(state_0, dict(state_1, state_0 -> state_1 transition probability at first two positions position))\n","    for state_1, cur_state in x_A1.items():\n","      self.A1[state_1] = self._calc_discrete_distribution(cur_state)\n","        \n","    self.A2 = {} # dict((state_t_2, state_t_1), dict(state_t, (state_t_2, state_t_1) -> state_t  transition probability in the whole sequence))\n","    for state_1, cur_state in x_A2.items():\n","      self.A2[state_1] = self._calc_discrete_distribution(cur_state)\n","\n","  def _calc_discrete_distribution(self, tokens: list):\n","    n = len(tokens)\n","    pdf = {}\n","    for i, token in enumerate(tokens):\n","      pdf[token] = pdf.get(token, 0) + 1\n","    \n","    for k, val in pdf.items():\n","      pdf[k] = val / n   \n","    return pdf\n","\n","\n","class MAPDiscreteSequenceGenerator(): # Maximum Posteriori\n","  def __init__(self, likelihood_model):\n","    self.model = likelihood_model\n","\n","  def generate(self, max_steps):\n","    assert max_steps > 0\n","    x_generated = []\n","    \n","    # generate token 0\n","    token_0_distribution = self.model.pi\n","    x_generated.append(self._cdf_inv(token_0_distribution))\n","\n","    # generate token 1\n","    if max_steps > 1:\n","      token_1_distribution = self.model.A1[x_generated[0]]\n","      next_token = self._cdf_inv(token_1_distribution)\n","      x_generated.append(self._cdf_inv(token_1_distribution))\n","      if self.model.seq_end is not None and next_token == self.model.seq_end:\n","        return x_generated\n","\n","    # generate tokens >= 2\n","    for t in range(2, max_steps):\n","      token_t_distribution = self.model.A2[(x_generated[-2], x_generated[-1])]\n","      next_token = self._cdf_inv(token_t_distribution)\n","      x_generated.append(next_token)\n","      if self.model.seq_end is not None and next_token == self.model.seq_end:\n","        break\n","    \n","    return x_generated\n","  \n","\n","  def _cdf_inv(self, discrete_distribution: dict):\n","    u = np.random.random()\n","    cdf = 0.0\n","    random_token = None\n","    for token, prob in discrete_distribution.items():\n","      cdf += prob\n","      if u < cdf:\n","        return token\n","    raise Exception('Unexpected line execution. Probably provided discrete distribution was not correct')"],"metadata":{"id":"8YvmX6yn0TSM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Processing"],"metadata":{"id":"DCptBj8BF9HY"}},{"cell_type":"code","source":["word2idx_train = build_word2idx(X_train, end_token=END_TOKEN)\n","V = len(word2idx_train)\n","print('vocab size V:', V)\n","\n","X_train_vectorized = np.array([tokenize(line, word2idx_train, end_token=END_TOKEN) for line in X_train], dtype=object)\n","X_train_vectorized[:3]"],"metadata":{"id":"7KHFdAONGY28","executionInfo":{"status":"ok","timestamp":1650375014432,"user_tz":-120,"elapsed":304,"user":{"displayName":"Marcin Figlewicz","userId":"11641752571162350955"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2eaa32a8-a0d3-4130-d5a9-53b45a489a78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["vocab size V: 2198\n"]},{"output_type":"execute_result","data":{"text/plain":["array([list([1, 2, 3, 4, 5, 6, 7, 1]),\n","       list([8, 9, 10, 11, 12, 13, 14, 1]),\n","       list([8, 15, 16, 17, 18, 10, 19, 1])], dtype=object)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["seq_model = SparseSecondOrderMarkovModel(V, seq_end=word2idx_train[END_TOKEN])\n","seq_model.fit(X_train_vectorized)\n","generator = MAPDiscreteSequenceGenerator(seq_model)"],"metadata":{"id":"pEkR7HXOn7Ym"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(' '.join(idx2word(generator.generate(3), word2idx_train)))\n","print(' '.join(idx2word(generator.generate(5), word2idx_train)))\n","print(' '.join(idx2word(generator.generate(10), word2idx_train)))\n","print(' '.join(idx2word(generator.generate(30), word2idx_train)))"],"metadata":{"id":"i2FvA_vUsp2n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650375015081,"user_tz":-120,"elapsed":11,"user":{"displayName":"Marcin Figlewicz","userId":"11641752571162350955"}},"outputId":"4d15d97a-94aa-45e0-96e6-e9ece5fc574c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["the darkest evening\n","and sometimes something seemed to\n","a moment he was two\n","up attic mother two\n"]}]}]}