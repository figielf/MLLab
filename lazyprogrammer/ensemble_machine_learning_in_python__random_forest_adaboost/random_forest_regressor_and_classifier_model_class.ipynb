{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"random_forest_regressor_and_classifier_model_class.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPLg469RU6a42Ll8hxZ6Ql/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":14,"metadata":{"id":"biMpfaLthUN6","executionInfo":{"status":"ok","timestamp":1644100970790,"user_tz":-60,"elapsed":408,"user":{"displayName":"Marcin Figlewicz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641752571162350955"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import r2_score\n","from datetime import datetime\n","from collections import Counter\n","\n","import sklearn.ensemble\n","from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","#from sklearn.ensemble import RandomForestRegressor\n","from sklearn.linear_model import LinearRegression, LogisticRegression\n","from sklearn.model_selection import train_test_split, cross_val_score"]},{"cell_type":"markdown","source":["# Utils"],"metadata":{"id":"tTktDF3EmY9h"}},{"cell_type":"markdown","source":["## Load housing data"],"metadata":{"id":"-RTgaykwpxdO"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)\n","\n","#def get_housing():\n","housing_df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/lazyprogrammer/data/housing.data', header=None, delim_whitespace=True)\n","housing_df.columns = [\n","    'crim', # numerical\n","    'zn', # numerical\n","    'nonretail', # numerical\n","    'river', # binary\n","    'nox', # numerical\n","    'rooms', # numerical\n","    'age', # numerical\n","    'dis', # numerical\n","    'rad', # numerical\n","    'tax', # numerical\n","    'ptratio', # numerical\n","    'b', # numerical\n","    'lstat', # numerical\n","    'medv', # numerical -- this is the target\n","  ]\n","\n","if housing_df.isna().sum().max() == 0:\n","  print('There is no NA values')\n","else:\n","  print(f'There are {housing_df.isna().sum().max()} NAs' )\n","\n","HOUSING_NUMERICAL_COLS = [\n","  'crim', # numerical\n","  'zn', # numerical\n","  'nonretail', # numerical\n","  'nox', # numerical\n","  'rooms', # numerical\n","  'age', # numerical\n","  'dis', # numerical\n","  'rad', # numerical\n","  'tax', # numerical\n","  'ptratio', # numerical\n","  'b', # numerical\n","  'lstat', # numerical\n","]\n","\n","#NO_TRANSFORM = ['river']\n","\n","housing_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":243},"id":"fBJhFuzanIgO","executionInfo":{"status":"ok","timestamp":1644100976344,"user_tz":-60,"elapsed":4809,"user":{"displayName":"Marcin Figlewicz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641752571162350955"}},"outputId":"b8ca921b-1fca-42b0-af4e-4b34be2650e2"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n","There is no NA values\n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-a366d63c-510b-4779-b637-ea0219c49215\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>crim</th>\n","      <th>zn</th>\n","      <th>nonretail</th>\n","      <th>river</th>\n","      <th>nox</th>\n","      <th>rooms</th>\n","      <th>age</th>\n","      <th>dis</th>\n","      <th>rad</th>\n","      <th>tax</th>\n","      <th>ptratio</th>\n","      <th>b</th>\n","      <th>lstat</th>\n","      <th>medv</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00632</td>\n","      <td>18.0</td>\n","      <td>2.31</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>6.575</td>\n","      <td>65.2</td>\n","      <td>4.0900</td>\n","      <td>1</td>\n","      <td>296.0</td>\n","      <td>15.3</td>\n","      <td>396.90</td>\n","      <td>4.98</td>\n","      <td>24.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.02731</td>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0</td>\n","      <td>0.469</td>\n","      <td>6.421</td>\n","      <td>78.9</td>\n","      <td>4.9671</td>\n","      <td>2</td>\n","      <td>242.0</td>\n","      <td>17.8</td>\n","      <td>396.90</td>\n","      <td>9.14</td>\n","      <td>21.6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.02729</td>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0</td>\n","      <td>0.469</td>\n","      <td>7.185</td>\n","      <td>61.1</td>\n","      <td>4.9671</td>\n","      <td>2</td>\n","      <td>242.0</td>\n","      <td>17.8</td>\n","      <td>392.83</td>\n","      <td>4.03</td>\n","      <td>34.7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.03237</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0</td>\n","      <td>0.458</td>\n","      <td>6.998</td>\n","      <td>45.8</td>\n","      <td>6.0622</td>\n","      <td>3</td>\n","      <td>222.0</td>\n","      <td>18.7</td>\n","      <td>394.63</td>\n","      <td>2.94</td>\n","      <td>33.4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.06905</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0</td>\n","      <td>0.458</td>\n","      <td>7.147</td>\n","      <td>54.2</td>\n","      <td>6.0622</td>\n","      <td>3</td>\n","      <td>222.0</td>\n","      <td>18.7</td>\n","      <td>396.90</td>\n","      <td>5.33</td>\n","      <td>36.2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a366d63c-510b-4779-b637-ea0219c49215')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a366d63c-510b-4779-b637-ea0219c49215 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a366d63c-510b-4779-b637-ea0219c49215');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      crim    zn  nonretail  river    nox  ...    tax  ptratio       b  lstat  medv\n","0  0.00632  18.0       2.31      0  0.538  ...  296.0     15.3  396.90   4.98  24.0\n","1  0.02731   0.0       7.07      0  0.469  ...  242.0     17.8  396.90   9.14  21.6\n","2  0.02729   0.0       7.07      0  0.469  ...  242.0     17.8  392.83   4.03  34.7\n","3  0.03237   0.0       2.18      0  0.458  ...  222.0     18.7  394.63   2.94  33.4\n","4  0.06905   0.0       2.18      0  0.458  ...  222.0     18.7  396.90   5.33  36.2\n","\n","[5 rows x 14 columns]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["def get_housing_data(test_size=0.3):\n","  class HousingDataTransformer:\n","    def __init__(self, numerical_columns):\n","      self.numerical_columns = numerical_columns\n","\n","    def fit(self, X):\n","      self.transformers = []\n","      self.features_dim = X.shape[1]\n","      for c in range(self.features_dim):\n","        if (X.columns[c] in self.numerical_columns):\n","          scaler = StandardScaler()\n","          scaler.fit(X.iloc[:,c].values.reshape(-1, 1))\n","          self.transformers.append(scaler)\n","        else:\n","          self.transformers.append(None)\n","    \n","    def transform(self, X):\n","      result = np.zeros((len(X), self.features_dim))\n","      i = 0\n","      for c in range(self.features_dim):\n","        scaler = self.transformers[c]\n","        if (X.columns[c] in self.numerical_columns):\n","          result[:,i] = scaler.transform(X.iloc[:,c].values.reshape(-1, 1)).flatten()\n","        else:\n","          result[:,i] = X.iloc[:,c]\n","        i += 1\n","      return result\n","\n","    def fit_transform(self, X):\n","      self.fit(X)\n","      return self.transform(X)\n","\n","  df = housing_df.copy()\n","  X = df.iloc[:,:-1]\n","  Y = df['medv']\n","  N = len(X)\n","  transformer = HousingDataTransformer(HOUSING_NUMERICAL_COLS)\n","\n","  if test_size > 0.0:\n","    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n","  else:\n","    X_train, X_test, Y_train, Y_test = X, None, Y, None\n","\n","  X_train_transformed = transformer.fit_transform(X_train)\n","  if X_test is not None:\n","    return X_train_transformed, transformer.transform(X_test), Y_train.values, Y_test.values\n","  else:\n","    return X_train_transformed, None, Y_train.values, None"],"metadata":{"id":"5S0KA_VWtea2","executionInfo":{"status":"ok","timestamp":1644100976345,"user_tz":-60,"elapsed":33,"user":{"displayName":"Marcin Figlewicz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641752571162350955"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## Load mushroom data"],"metadata":{"id":"2Euky0sutoMj"}},{"cell_type":"code","source":["mushroom_df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/lazyprogrammer/data/agaricus-lepiota.data', header=None)\n","\n","if mushroom_df.isna().sum().max() == 0:\n","  print('There is no NA values')\n","else:\n","  print(f'There are {mushroom_df.isna().sum().max()} NAs' )\n","\n","MUSHROOM_NUMERICAL_COLS = ()\n","MUSHROOM_CATEGORICAL_COLS = np.arange(22) + 1 # 1..22 inclusive\n","\n","mushroom_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225},"id":"0goBIGu6tx1V","executionInfo":{"status":"ok","timestamp":1644100976346,"user_tz":-60,"elapsed":30,"user":{"displayName":"Marcin Figlewicz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641752571162350955"}},"outputId":"5b34e3dc-e710-4c33-e256-fbda5ea2d9d1"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["There is no NA values\n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-a7683747-91ca-4f09-9ce0-64e9a9ac2e72\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>p</td>\n","      <td>x</td>\n","      <td>s</td>\n","      <td>n</td>\n","      <td>t</td>\n","      <td>p</td>\n","      <td>f</td>\n","      <td>c</td>\n","      <td>n</td>\n","      <td>k</td>\n","      <td>e</td>\n","      <td>e</td>\n","      <td>s</td>\n","      <td>s</td>\n","      <td>w</td>\n","      <td>w</td>\n","      <td>p</td>\n","      <td>w</td>\n","      <td>o</td>\n","      <td>p</td>\n","      <td>k</td>\n","      <td>s</td>\n","      <td>u</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>e</td>\n","      <td>x</td>\n","      <td>s</td>\n","      <td>y</td>\n","      <td>t</td>\n","      <td>a</td>\n","      <td>f</td>\n","      <td>c</td>\n","      <td>b</td>\n","      <td>k</td>\n","      <td>e</td>\n","      <td>c</td>\n","      <td>s</td>\n","      <td>s</td>\n","      <td>w</td>\n","      <td>w</td>\n","      <td>p</td>\n","      <td>w</td>\n","      <td>o</td>\n","      <td>p</td>\n","      <td>n</td>\n","      <td>n</td>\n","      <td>g</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>e</td>\n","      <td>b</td>\n","      <td>s</td>\n","      <td>w</td>\n","      <td>t</td>\n","      <td>l</td>\n","      <td>f</td>\n","      <td>c</td>\n","      <td>b</td>\n","      <td>n</td>\n","      <td>e</td>\n","      <td>c</td>\n","      <td>s</td>\n","      <td>s</td>\n","      <td>w</td>\n","      <td>w</td>\n","      <td>p</td>\n","      <td>w</td>\n","      <td>o</td>\n","      <td>p</td>\n","      <td>n</td>\n","      <td>n</td>\n","      <td>m</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>p</td>\n","      <td>x</td>\n","      <td>y</td>\n","      <td>w</td>\n","      <td>t</td>\n","      <td>p</td>\n","      <td>f</td>\n","      <td>c</td>\n","      <td>n</td>\n","      <td>n</td>\n","      <td>e</td>\n","      <td>e</td>\n","      <td>s</td>\n","      <td>s</td>\n","      <td>w</td>\n","      <td>w</td>\n","      <td>p</td>\n","      <td>w</td>\n","      <td>o</td>\n","      <td>p</td>\n","      <td>k</td>\n","      <td>s</td>\n","      <td>u</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>e</td>\n","      <td>x</td>\n","      <td>s</td>\n","      <td>g</td>\n","      <td>f</td>\n","      <td>n</td>\n","      <td>f</td>\n","      <td>w</td>\n","      <td>b</td>\n","      <td>k</td>\n","      <td>t</td>\n","      <td>e</td>\n","      <td>s</td>\n","      <td>s</td>\n","      <td>w</td>\n","      <td>w</td>\n","      <td>p</td>\n","      <td>w</td>\n","      <td>o</td>\n","      <td>e</td>\n","      <td>n</td>\n","      <td>a</td>\n","      <td>g</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7683747-91ca-4f09-9ce0-64e9a9ac2e72')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a7683747-91ca-4f09-9ce0-64e9a9ac2e72 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a7683747-91ca-4f09-9ce0-64e9a9ac2e72');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["  0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22\n","0  p  x  s  n  t  p  f  c  n  k  e  e  s  s  w  w  p  w  o  p  k  s  u\n","1  e  x  s  y  t  a  f  c  b  k  e  c  s  s  w  w  p  w  o  p  n  n  g\n","2  e  b  s  w  t  l  f  c  b  n  e  c  s  s  w  w  p  w  o  p  n  n  m\n","3  p  x  y  w  t  p  f  c  n  n  e  e  s  s  w  w  p  w  o  p  k  s  u\n","4  e  x  s  g  f  n  f  w  b  k  t  e  s  s  w  w  p  w  o  e  n  a  g"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["from future.utils import iteritems\n","\n","class MushroomDataTransformer:\n","  def __init__(self, numerical_columns, categorical_columns):\n","    self.numerical_columns = numerical_columns\n","    self.categorical_columns = categorical_columns\n","\n","  def fit(self, df):\n","    self.labelEncoders = {}\n","    self.scalers = {}\n","    for col in self.numerical_columns:\n","      scaler = StandardScaler()\n","      scaler.fit(df[col].reshape(-1, 1))\n","      self.scalers[col] = scaler\n","\n","    for col in self.categorical_columns:\n","      encoder = LabelEncoder()\n","      # in case the train set does not have 'missing' value but test set does\n","      values = df[col].tolist()\n","      values.append('missing')\n","      encoder.fit(values)\n","      self.labelEncoders[col] = encoder\n","\n","    # find dimensionality\n","    self.D = len(self.numerical_columns)\n","    for col, encoder in iteritems(self.labelEncoders):\n","      self.D += len(encoder.classes_)\n","    print(\"dimensionality:\", self.D)\n","\n","  def transform(self, df):\n","    N, _ = df.shape\n","    X = np.zeros((N, self.D))\n","    i = 0\n","    for col, scaler in iteritems(self.scalers):\n","      X[:,i] = scaler.transform(df[col].values.reshape(-1, 1)).flatten()\n","      i += 1\n","\n","    for col, encoder in iteritems(self.labelEncoders):\n","      # print \"transforming col:\", col\n","      K = len(encoder.classes_)\n","      X[np.arange(N), encoder.transform(df[col]) + i] = 1\n","      i += K\n","    return X\n","\n","  def fit_transform(self, df):\n","    self.fit(df)\n","    return self.transform(df)\n"],"metadata":{"id":"x8DxEAltwKZA","executionInfo":{"status":"ok","timestamp":1644100976348,"user_tz":-60,"elapsed":25,"user":{"displayName":"Marcin Figlewicz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641752571162350955"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def replace_missing(df, numerical_columns, categorical_columns, special_missing_category='missing'):\n","  # standard method of replacement for numerical columns is median\n","  for col in numerical_columns:\n","    if np.any(df[col].isnull()):\n","      med = np.median(df[ col ][ df[col].notnull() ])\n","      df.loc[ df[col].isnull(), col ] = med\n","\n","  # set a special value = 'missing'\n","  for col in categorical_columns:\n","    if np.any(df[col].isnull()):\n","      print(col)\n","      df.loc[ df[col].isnull(), col ] = special_missing_category\n","\n","\n","def get_mushroom_data(test_size=0.3):\n","  df = mushroom_df.copy()\n","  # replace label column: e/p --> 0/1, e = edible = 0, p = poisonous = 1\n","  df[0] = df.apply(lambda row: 0 if row[0] == 'e' else 1, axis=1)\n","\n","  replace_missing(df, MUSHROOM_NUMERICAL_COLS, MUSHROOM_CATEGORICAL_COLS)\n","  transformer = MushroomDataTransformer(MUSHROOM_NUMERICAL_COLS, MUSHROOM_CATEGORICAL_COLS)\n","\n","  X = df\n","  Y = df[0]\n","  if test_size > 0.0:\n","    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n","  else:\n","    X_train, X_test, Y_train, Y_test = X, None, Y, None\n","\n","  X_train_transformed = transformer.fit_transform(X_train)\n","  if X_test is not None:\n","    return X_train_transformed, transformer.transform(X_test), Y_train.values, Y_test.values\n","  else:\n","    return X_train_transformed, None, Y_train.values, None"],"metadata":{"id":"o_dQnRShu0no","executionInfo":{"status":"ok","timestamp":1644100976349,"user_tz":-60,"elapsed":23,"user":{"displayName":"Marcin Figlewicz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641752571162350955"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["## Util functions"],"metadata":{"id":"zEZJ4jS4Edn0"}},{"cell_type":"code","source":["from sklearn.model_selection import KFold\n","\n","def my_cross_val_score(estimator, X, Y, cv, shuffle=False, random_state=None):\n","  N = len(Y)\n","  if isinstance(cv, KFold):\n","    kf = cv\n","  elif isinstance(cv, int):\n","    kf = KFold(n_splits=cv, random_state=random_state, shuffle=shuffle)\n","  else:\n","    raise Exception(f'cv param can be int or KFold but was {type(cv)} and had value of {cv}')\n","\n","  kf_scores = []\n","  for train_index, test_index in kf.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    Y_train, Y_test = Y[train_index], Y[test_index]\n","\n","    estimator.fit(X_train, Y_train)\n","    estimator.score(X_test, Y_test)\n","    kf_scores.append(estimator.score(X_test, Y_test))\n","  return np.array(kf_scores)"],"metadata":{"id":"uiIWq3KhEhQU","executionInfo":{"status":"ok","timestamp":1644100976349,"user_tz":-60,"elapsed":22,"user":{"displayName":"Marcin Figlewicz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641752571162350955"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["# Ramdom Forest model class"],"metadata":{"id":"vj7QoQSwhEqf"}},{"cell_type":"markdown","source":["## Decision tree class from Superviced Machine Learning course"],"metadata":{"id":"sSaX-CCvmrtu"}},{"cell_type":"code","source":["def binary_entropy(y):\n","    # assume y is binary - 0 or 1\n","    N = len(y)\n","    s1 = (y == 1).sum()\n","    if 0 == s1 or N == s1:\n","        return 0\n","    p1 = float(s1) / N\n","    p0 = 1 - p1\n","    return -p0 * np.log2(p0) - p1 * np.log2(p1)\n","\n","\n","class BinaryTreeNode:\n","    def __init__(self, depth, max_depth=None, max_bucket_size=None, trace_logs=True):\n","        # print(f'Creating new node with depth={depth}')\n","        self.depth = depth\n","        self.max_depth = max_depth\n","        self.max_bucket_size = max_bucket_size\n","        self.trace_logs = trace_logs\n","        if self.max_depth is not None and self.max_depth < self.depth:\n","            raise Exception(f'depth > max_depth:{depth > max_depth}, depth={depth}, max_depth={max_depth}')\n","        self.split_column_idx = None\n","        self.split_value = None\n","        self.left_child = None\n","        self.right_child = None\n","        self.prediction = None\n","        self.information_gain = None\n","\n","    def fit(self, X, Y):\n","        #if (self.trace_logs == True):\n","            #print(f'fit (depth:{self.depth}) - Start fit')\n","            # print(f'fit (depth:{self.depth}) - X:{X}')\n","            # print(f'fit (depth:{self.depth}) - Y:{Y}')\n","        if (self._is_fitted() == False):\n","            if (self._can_split(Y)):\n","                # print('Is allowed to split')\n","                split_column, self.split_value, self.information_gain = self._find_best_split(X, Y)\n","                if (self.trace_logs == True):\n","                    print(f'fit (depth:{self.depth}) - (best_split_col, best_split_value, max_ig):{(split_column, self.split_value, self.information_gain)}')\n","                if (split_column is None):\n","                    #print(f'fit (depth:{self.depth}) - no splits found, will make this node a leaf')\n","                    self.prediction = self._calc_prediction(Y)\n","                    # print(f'fit (depth:{self.depth}) - Leaf on level {self.depth}, calculated prediction={self.prediction}')\n","                    return\n","\n","                self.split_column_idx = int(split_column)\n","                left_split_mask = self._get_left_split_mask(X[:, self.split_column_idx], self.split_value)\n","                X_left, X_right = X[left_split_mask], X[~left_split_mask]\n","                Y_left, Y_right = Y[left_split_mask], Y[~left_split_mask]\n","                # print(f'fit (depth:{self.depth}) - Y_left len:{len(Y_left)}, Y_right len:{len(Y_right)}')\n","                # print(f'fit (depth:{self.depth}) - Y before split len:{len(Y)}, Y:{Y}')\n","                self.left_child = self._make_child_node(self.depth + 1, self.max_depth, self.max_bucket_size,\n","                                                        trace_logs=self.trace_logs)\n","                self.left_child.fit(X_left, Y_left)\n","                self.right_child = self._make_child_node(self.depth + 1, self.max_depth, self.max_bucket_size,\n","                                                         trace_logs=self.trace_logs)\n","                self.right_child.fit(X_right, Y_right)\n","            else:\n","                #print(f'fit (depth:{self.depth}) - Is not allowed to split')\n","                #print('fit (depth:{self.depth}) - Y:', Y)\n","                self.prediction = self._calc_prediction(Y)\n","                # print(f'fit (depth:{self.depth}) - Calculated prediction={self.prediction}')\n","\n","    def predict(self, X):\n","        result = np.zeros(len(X))\n","        #print(f'predict - node level={self.depth}')\n","        if (self._is_leaf() == True):\n","            #print('Node has not childrens')\n","            #print('predict - self.prediction:', self.prediction)\n","            return self.prediction\n","        left_split_mask = self._get_left_split_mask(X[:, self.split_column_idx], self.split_value)\n","\n","        left_predictions = self.left_child.predict(X[left_split_mask])\n","        right_predictions = self.right_child.predict(X[~left_split_mask])\n","        #print(f'predict - left_predictions:{left_predictions}, left_split_mask:{left_split_mask}')\n","        #print(f'predict - right_predictions:{right_predictions}, right_split_mask:{~left_split_mask}')\n","\n","        #print('predict - left_split_mask:', left_split_mask)\n","        result[left_split_mask] = left_predictions\n","        result[~left_split_mask] = right_predictions\n","        #print('predict - result:', result)\n","        return result\n","\n","    def get_importance(self):\n","        # tabs = '\\t'*self.depth\n","        # print(f'{tabs}get_importance - node level {self.depth}')\n","        if (self._is_fitted() == False):\n","            raise Exception(f'Node on level {self.depth} is not fitted yet')\n","        if (self._is_leaf()):  # no split no gain\n","            return np.array([(0, 0)])\n","        left_importance = self.left_child.get_importance()\n","        # print(f'{tabs}get_importance on level {self.depth} - left_importance:{left_importance}')\n","        right_importance = self.right_child.get_importance()\n","        # print(f'{tabs}get_importance on level {self.depth} - right_importance:{right_importance}')\n","        return self._calc_node_level_total_importance(left_importance, right_importance)\n","\n","    def self_make_child_node(self, depth, max_depth=None, max_bucket_size=None, trace_logs=True):\n","        raise NotImplementedError()\n","\n","    def _calc_prediction(self, y):\n","        raise NotImplementedError()\n","\n","    def _calc_node_cost(self, y):\n","        raise NotImplementedError()\n","\n","    def _calc_information_gain(self, y, split_mask):\n","        y0 = y[split_mask]\n","        y1 = y[~split_mask]\n","        # print(f'_calc_information_gain - y0:{y0}')\n","        # print(f'_calc_information_gain - y1:{y1}')\n","        N_0 = len(y0)\n","        N_1 = len(y1)\n","        N = N_0 + N_1\n","        if (N_0 == 0 or N_1 == 0):\n","            # print(f'_calc_information_gain - one leg (left:{N_0}, right:{N_1}) is lenght of 0 so 0 is returned as information gain')\n","            return 0\n","        # print(f'_calc_information_gain - self._calc_node_cost(y):{self._calc_node_cost(y)}, self._calc_node_cost(y0):{self._calc_node_cost(y0)}, self._calc_node_cost(y1):{self._calc_node_cost(y1)}')\n","        return self._calc_node_cost(y) - (N_0 * self._calc_node_cost(y0) + N_1 * self._calc_node_cost(y1)) / N\n","\n","    def _calc_node_level_total_importance(self, left_child_importance, right_child_importance):\n","        tabs = '\\t' * self.depth\n","        importances = np.concatenate(\n","            (left_child_importance, right_child_importance, np.array([[self.split_column_idx, self.information_gain]])))\n","        # print(f'{tabs}_calc_node_level_total_importance on level {self.depth} - not summed importances:')\n","        # print(importances)\n","        importances_df = pd.DataFrame(importances, columns=['col_idx', 'information_gain'])\n","        importance = importances_df.groupby('col_idx', as_index=False).sum().values\n","        # print(f'{tabs}_calc_node_level_total_importance on level {self.depth} - summed importance:')\n","        # print(importance)\n","        return importance\n","\n","    def _is_fitted(self):\n","        has_no_split_details = self.split_column_idx is None and self.split_value is None and self.left_child is None and self.right_child is None\n","        if (self.prediction is None and has_no_split_details):  # not a leaf neither splitted\n","            return False\n","        if (self.prediction is not None and has_no_split_details == True):  # is leaf\n","            return True\n","        has_split_details = self.split_column_idx is not None and self.split_value is not None and self.left_child is not None and self.right_child is not None\n","        if (self.prediction is None and has_split_details == True):  # is splitted\n","            return True\n","        raise Exception(f'There are conflicting values in self.prediction and other attributes related to node split')\n","\n","    def _is_leaf(self):\n","        if (self._is_fitted()):\n","            return self.prediction is not None\n","        return False\n","\n","    def _can_split(self, y):\n","        # True if all below\n","        # 1. depth not bigger than allowed => self.depth <= self.max_depth\n","        # 2. num of obserwations bigger than requested => len(y) > self.max_bucket_size\n","        # 3. there is any variation in labels => (len(set(y)) == 1) > 1\n","        allowed = True\n","        # print('should_try_split - result:', allowed)\n","        if (self.max_depth is not None):\n","            # print(f'self.max_depth is not None and node.depth <= self.max_depth={node.depth <= self.max_depth}, node.depth={node.depth}, self.max_depth={self.max_depth}')\n","            allowed = allowed and self.depth < self.max_depth\n","        # print('should_try_split - result:', allowed)\n","        if (self.max_bucket_size is not None):\n","            # print(f'self.max_bucket_size is not None and bucket_size > self.max_bucket_size={bucket_size > self.max_bucket_size}, node.bucket_size={bucket_size}, self.max_bucket_size={self.max_bucket_size}')\n","            allowed = allowed and len(y) > self.max_bucket_size\n","        if (len(y) == 1 or len(set(y)) == 1):\n","            return False\n","        # print('should_try_split - result:', allowed)\n","        return allowed\n","\n","    def _find_best_split(self, x, y):\n","        # print(f'_find_best_split - x:{x}')\n","        splits = self._get_split_candidates(x, y)\n","        # print('_find_best_split - split candicates:', x)\n","        # print('_find_best_split - split candicates:', pd.DataFrame(splits, columns=['column_idx', 'split_value', 'ig']))\n","        if (len(splits) == 0):\n","            return (None, None, None)\n","        return splits[np.argmax(splits[:, 2])]\n","\n","    def _get_split_candidates(self, x, y):\n","        splits = []\n","        for i in range(x.shape[1]):\n","            x_col = x[:, i]\n","            if (len(set(x_col)) == 1):\n","                # print(f'_find_all_splits - all split column {i} valueas are same (={x[:,i][0]}) and should no split further')\n","                continue\n","            sort_idx = np.argsort(x_col)\n","            x_col_sorted = x_col[sort_idx]\n","            y_sorted = y[sort_idx]\n","            steps_idx = self._get_steps(y_sorted)\n","            # print(f'_find_all_splits - column={i}, steps_idx:{steps_idx}, x_col_sorted:{x_col_sorted}, y_sorted:{y_sorted}')\n","            for s_idx in steps_idx:\n","                split_point = (x_col_sorted[s_idx] + x_col_sorted[s_idx + 1]) / 2.0\n","                # print('_find_all_splits - split_point:', split_point)\n","                left_split_mask = self._get_left_split_mask(x_col, split_point)\n","                ig = self._calc_information_gain(y, left_split_mask)\n","                if (ig > 0.0):\n","                    # print('_find_all_splits - calculated information gain:', ig)\n","                    splits.append([i, split_point, ig])\n","                # else:\n","                    # print(f'_find_all_splits - calculated information gain is {0} so will skip point {split_point} this as candicate')\n","        return np.array(splits)\n","\n","    def _get_left_split_mask(self, x, split_by_value):\n","        # print(f'_get_left_split_mask - split_by_value:{split_by_value}')\n","        # print(f'_get_left_split_mask - x:{x}')\n","        left_split_mask = x < split_by_value\n","        # print('_get_split_mask - left_split_mask:', left_split_mask)\n","        return left_split_mask\n","\n","    def _get_steps(self, y):\n","        return np.nonzero(y[:-1] != y[1:])[0]\n","\n","\n","class BinaryTreeClassifierNode(BinaryTreeNode):\n","    def __init__(self, depth, max_depth=None, max_bucket_size=None, trace_logs=True):\n","        super().__init__(depth, max_depth, max_bucket_size, trace_logs)\n","        self.prediction_features = None\n","\n","    def _calc_prediction(self, y):\n","        if (len(y) == 1 or len(set(y)) == 1):\n","            return y[0]\n","        return int(np.round(y.mean()))\n","\n","    def _calc_node_cost(self, y):\n","        return binary_entropy(y)\n","\n","    def _make_child_node(self, depth, max_depth=None, max_bucket_size=None, trace_logs=True):\n","        return BinaryTreeClassifierNode(depth, max_depth, max_bucket_size, trace_logs)\n","\n","\n","class BinaryTreeRegressorNode(BinaryTreeNode):\n","    def __init__(self, depth, max_depth=None, max_bucket_size=None, trace_logs=True):\n","        super().__init__(depth, max_depth, max_bucket_size, trace_logs)\n","        self.prediction_features = None\n","\n","    def _calc_prediction(self, y):\n","        if (len(y) == 1 or len(set(y)) == 1):\n","            result = y[0]\n","        result =  y.mean()\n","        print (f'_calc_prediction - Calculated leaf prediction based on {len(y)} targets (y:{y}). Prediction={result}')\n","        if (np.isnan(result)):\n","            raise Exception(f'_calc_prediction calculated nan for the leaf, result={result}, len(y)={len(y)}')\n","        return result\n","\n","    def _calc_node_cost(self, y):\n","        return np.var(y)\n","\n","    def _make_child_node(self, depth, max_depth=None, max_bucket_size=None, trace_logs=True):\n","        return BinaryTreeRegressorNode(depth, max_depth, max_bucket_size, trace_logs)\n","\n","\n","class BinaryTreeBase():\n","    def __init__(self, max_depth=10, max_bucket_size=10, trace_logs=True):\n","        self.max_depth = max_depth\n","        self.max_bucket_size = max_bucket_size\n","        self.trace_logs = trace_logs\n","\n","    def predict(self, X):\n","        return self.head.predict(X)\n","\n","    def score(self, X, Y):\n","        predictions = self.predict(X)\n","        return np.mean(predictions == Y)\n","\n","    def get_importance(self):\n","        return self.head.get_importance()\n","\n","\n","class BinaryTreeClassifier(BinaryTreeBase):\n","    def __init__(self, max_depth=10, max_bucket_size=10, trace_logs=True):\n","        super().__init__(max_depth, max_bucket_size, trace_logs)\n","\n","    def fit(self, X, Y):\n","        self.head = BinaryTreeClassifierNode(1, self.max_depth, self.max_bucket_size, trace_logs=self.trace_logs)\n","        self.head.fit(X, Y)\n","\n","\n","class BinaryTreeRegressor(BinaryTreeBase):\n","    def __init__(self, max_depth=10, max_bucket_size=10, trace_logs=True):\n","        super().__init__(max_depth, max_bucket_size, trace_logs)\n","\n","    def fit(self, X, Y):\n","        self.head = BinaryTreeRegressorNode(1, self.max_depth, self.max_bucket_size, trace_logs=self.trace_logs)\n","        self.head.fit(X, Y)\n"],"metadata":{"id":"kQX4X8A_5WjJ","executionInfo":{"status":"ok","timestamp":1644100976971,"user_tz":-60,"elapsed":643,"user":{"displayName":"Marcin Figlewicz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641752571162350955"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["\n","class BaggingBase():\n","  def __init__(self, model_factory_fun, n_models, sample_size=None):\n","    self.model_factory_fun = model_factory_fun\n","    self.n_models = n_models\n","    self.sample_size = sample_size\n","\n","  def fit(self, X, Y):\n","    N = len(Y)\n","    b_sample_size = self.sample_size if self.sample_size is not None else N\n","    self.models = []\n","    for k in range(self.n_models):\n","      print(f'Fitting {k+1}-th model out of {self.n_models}')\n","      sample_idx = np.random.choice(N, size=b_sample_size, replace=True)\n","      #sample_idx = np.random.choice(N, size=b_sample_size, replace=False)\n","\n","      Xb = X[sample_idx]\n","      Yb = Y[sample_idx]\n","\n","      model = self.model_factory_fun()\n","      model.fit(Xb, Yb)\n","      self.models.append(model)\n","\n","  def _predictions(self, X):\n","    predictions = []\n","    for model in self.models:\n","      pred = model.predict(X)\n","      predictions.append(pred)\n","    return np.array(predictions)\n","\n","\n","class BaggingClassifier(BaggingBase):\n","  def __init__(self, model_factory_fun, n_models, sample_size=None):\n","    super().__init__(model_factory_fun, n_models, sample_size)\n","\n","  def predict(self, X):\n","    predictions = super()._predictions(X)\n","    if (len(predictions[0].shape) > 1):\n","      raise Exception(\n","        f'Only non sparse prediction output is supported. Shape of single model prediction is:{predictions[0].shape}')\n","    else:\n","      N = len(X)\n","      Y_hat = np.zeros(N)\n","      for i in range(N):\n","        Y_hat[i] = self._most_frequent(predictions[:, i])\n","      return Y_hat\n","\n","  def score(self, X, Y):\n","    Y_hat = self.predict(X)\n","    return (Y_hat == Y).mean()\n","\n","  def _most_frequent(self, array1d):\n","    occurence_count = Counter(array1d)\n","    return occurence_count.most_common(1)[0][0]\n","\n","\n","class BaggingRegressor(BaggingBase):\n","  def __init__(self, model_factory_fun, n_models, sample_size=None):\n","    super().__init__(model_factory_fun, n_models, sample_size)\n","\n","  def predict(self, X):\n","    predictions = super()._predictions(X)\n","    # print('BaggingRegressor - predict - predictions:', predictions)\n","    return predictions.mean(axis=0)\n","\n","  def score(self, X, Y):\n","    Y_hat = self.predict(X)\n","    return r2_score(Y_hat, Y)"],"metadata":{"id":"NrnT_iya8YrP","executionInfo":{"status":"ok","timestamp":1644100976972,"user_tz":-60,"elapsed":11,"user":{"displayName":"Marcin Figlewicz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641752571162350955"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["class RandomForestBinaryTreeNode(BinaryTreeNode):\n","  def __init__(self, depth, max_depth=None, max_bucket_size=None, n_features=None, trace_logs=True):\n","    self.n_features = n_features\n","    super().__init__(depth, max_depth, max_bucket_size, trace_logs)\n","    self.split_features_subset = None\n","\n","  \n","  def _find_best_split(self, x, y):\n","    if (self.n_features is not None):\n","      if (self.n_features > x.shape[1]):\n","        raise Exception(f'n_features={self.n_features} can not be bigger than number of features in X={x.shape[1]}')\n","      self.split_features_subset = np.sort(np.random.choice(x.shape[1], size=self.n_features, replace=False))\n","      # print(f'_find_best_split - self.split_features_subset:{self.split_features_subset} out of all x features x.shape[1]:{x.shape[1]}')\n","      best_split = super()._find_best_split(x[:, self.split_features_subset], y)\n","      # print(f'_find_best_split - type(best_split):{type(best_split)}, best_split:{best_split}')\n","      best_split_col_idx, best_split_point, best_ig = best_split[0], best_split[1], best_split[2]\n","      if (best_split_col_idx is None):\n","        return (None, None, None)\n","      # print(f'_find_best_split - self.split_features_subset[best_split_col_idx]:{self.split_features_subset[int(best_split_col_idx)]}')\n","      return [self.split_features_subset[int(best_split_col_idx)], best_split_point, best_ig]\n","    else:\n","      return super()._find_best_split(x, y)\n","\n","\n","class RandomForestTreeClassifierNode(RandomForestBinaryTreeNode):\n","  def __init__(self, depth, max_depth=None, max_bucket_size=None, n_features=None, trace_logs=True):\n","    super().__init__(depth, max_depth, max_bucket_size, n_features, trace_logs)\n","\n","  def _calc_prediction(self, y):\n","    #print('_calc_prediction - len(y):', len(y))\n","    if (len(y) == 1 or len(set(y)) == 1):\n","      result = y[0]\n","    result = int(np.round(y.mean()))\n","    #print (f'_calc_prediction - Calculated leaf prediction based on {len(y)} targets (y:{y}). Prediction={result}')\n","    if (np.isnan(result)):\n","      raise Exception(f'_calc_prediction calculated nan for the leaf, result={result}, len(y)={len(y)}')\n","    return result\n","\n","  def _calc_node_cost(self, y):\n","    return binary_entropy(y)\n","\n","  def _make_child_node(self, depth, max_depth=None, max_bucket_size=None, trace_logs=True):\n","    return RandomForestTreeClassifierNode(depth, max_depth, max_bucket_size, self.n_features, trace_logs)\n","\n","\n","class RandomForestTreeRegressorNode(RandomForestBinaryTreeNode):\n","  def __init__(self, depth, max_depth=None, max_bucket_size=None, n_features=None, trace_logs=True):\n","    self.n_features = n_features\n","    super().__init__(depth, max_depth, max_bucket_size, n_features, trace_logs)\n","\n","  def _calc_prediction(self, y):\n","    if (len(y) == 1 or len(set(y)) == 1):\n","      result = y[0]\n","    result = y.mean()\n","    #print (f'_calc_prediction - Calculated leaf prediction based on {len(y)} targets (y:{y}). Prediction={result}')\n","    if (np.isnan(result)):\n","      raise Exception(f'_calc_prediction calculated nan for the leaf, result={result}, len(y)={len(y)}')\n","    return result\n","\n","  def _calc_node_cost(self, y):\n","    if (y is None or len(y) <= 1):\n","      return 0\n","    return np.var(y)\n","\n","  def _make_child_node(self, depth, max_depth=None, max_bucket_size=None, trace_logs=True):\n","    return RandomForestTreeRegressorNode(depth, max_depth, max_bucket_size, self.n_features, trace_logs)\n","\n","\n","class BinaryTreeForRandomForestClassifier(BinaryTreeBase):\n","  def __init__(self, max_depth=10, max_bucket_size=10, n_features=None, trace_logs=True):\n","    self.n_features = n_features\n","    super().__init__(max_depth, max_bucket_size, trace_logs)\n","  \n","  def fit(self, X, Y):\n","    self.head = RandomForestTreeClassifierNode(1, self.max_depth, self.max_bucket_size, self.n_features, trace_logs=self.trace_logs)\n","    self.head.fit(X, Y)\n","\n","\n","class BinaryTreeForRandomForestRegressor(BinaryTreeBase):\n","  def __init__(self, max_depth=10, max_bucket_size=10, n_features=None, trace_logs=True):\n","    self.n_features = n_features\n","    super().__init__(max_depth, max_bucket_size, trace_logs)\n","  \n","  def fit(self, X, Y):\n","    self.head = RandomForestTreeRegressorNode(1, self.max_depth, self.max_bucket_size, self.n_features, trace_logs=self.trace_logs)\n","    self.head.fit(X, Y)\n","\n","\n","class RandomForestClassifier():\n","  def __init__(self, n_models, sample_size=None, n_features=None, max_depth=None, max_bucket_size=None, trace_logs=True):\n","    self.n_models = n_models\n","    self.sample_size = sample_size\n","    self.n_features = n_features\n","    self.max_depth = max_depth\n","    self.max_bucket_size = max_bucket_size\n","    self.trace_logs = trace_logs\n","  \n","  def fit(self, X, Y):\n","    model_factory = lambda : BinaryTreeForRandomForestClassifier(self.max_depth, self.max_bucket_size, self.n_features, self.trace_logs)\n","    self.bagged_tree = BaggingClassifier(model_factory, self.n_models, self.sample_size)\n","    self.bagged_tree.fit(X, Y)\n","  \n","  def predict(self, X):\n","    predictions = self.bagged_tree.predict(X)\n","    #print('predictions.shape', predictions.shape)\n","    #print('predict - super()._predictions(X):', predictions)\n","    return predictions\n","    \n","  def score(self, X, Y):\n","    return self.bagged_tree.score(X, Y)\n","\n","\n","class RandomForestRegressor():\n","  def __init__(self, n_models, sample_size=None, n_features=None, max_depth=None, max_bucket_size=None, trace_logs=True):\n","    self.n_models = n_models\n","    self.sample_size = sample_size\n","    self.n_features = n_features\n","    self.max_depth = max_depth\n","    self.max_bucket_size = max_bucket_size\n","    self.trace_logs = trace_logs\n","  \n","  def fit(self, X, Y):\n","    model_factory = lambda : BinaryTreeForRandomForestRegressor(self.max_depth, self.max_bucket_size, self.n_features, self.trace_logs)\n","    self.bagged_tree = BaggingRegressor(model_factory, self.n_models, self.sample_size)\n","    self.bagged_tree.fit(X, Y)\n","  \n","  def predict(self, X):\n","    predictions = self.bagged_tree.predict(X)\n","    return predictions\n","    \n","  def score(self, X, Y):\n","    return self.bagged_tree.score(X, Y)"],"metadata":{"id":"OCswEi274ToM","executionInfo":{"status":"ok","timestamp":1644100976973,"user_tz":-60,"elapsed":10,"user":{"displayName":"Marcin Figlewicz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641752571162350955"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["# Test Random Forest model"],"metadata":{"id":"RJZVsD3gbgJW"}},{"cell_type":"markdown","source":["## Simple sanity test"],"metadata":{"id":"GfqndErlANsp"}},{"cell_type":"code","source":["def _test():\n","  q=np.array([1,4,-999,1,2,8,19,2,7,16,3,1,34,12,19,2,1111,55,3,55,34,9,9,27788,657456456456,16,77,1,2,8,19,2,1111,16,3,1,34,12,19,2,12,55,3,55,34,9,9,5645,1,16,-1239,1,2,8,19,2,8,16,3,1,34,12,19,2,1111,55,3,55,34,9,9,27788,1,16,-1239,1,2,8,19,2,1111,16,3,1,34,12,19,2,1111,55,3,55,34,9,9,27788]).reshape((-1,3))\n","  qy=np.array([0,1,1,1,0,1,0,1,0,1,1,1,0,1,0,1,0,1,1,1,0,1,0,1,0,1,1,1,0,1,0,1])\n","\n","  #q=np.array([7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7]).reshape((-1,2))\n","  #qy=np.array([0,1,1,1,0,0,1,1,1,0])\n","\n","  #model = RandomForestRegressor(n_models=4)\n","  model = RandomForestRegressor(n_models=100, n_features=1, trace_logs=False)\n","  model.fit(q, qy)\n","\n","  #q_pred=np.array([1,1,1,1,1]).reshape((-1,1))\n","  #qy_pred=np.array([0,1,1,1,])\n","\n","  #q_pred=np.array([1,1,19,1,19,1,19,1,19]).reshape((-1,1))\n","\n","  #print(q+0.5)\n","  #predictions = model.predict(q+.5)\n","  #print('final predictions.shape', predictions.shape)\n","  \n","  #print(predictions == qy)\n","  \n","  print(\"test my score forest:\", model.score(q+.5, qy))\n","\n","  #imp = model.get_importance()\n","  #print(f'test result imporatnce:{imp}')\n","\n","_test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n5nRJJPzAOT1","executionInfo":{"status":"ok","timestamp":1644100977512,"user_tz":-60,"elapsed":547,"user":{"displayName":"Marcin Figlewicz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641752571162350955"}},"outputId":"53d274f4-12e9-4684-8edd-3df70fc73ada"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 1-th model out of 100\n","Fitting 2-th model out of 100\n","Fitting 3-th model out of 100\n","Fitting 4-th model out of 100\n","Fitting 5-th model out of 100\n","Fitting 6-th model out of 100\n","Fitting 7-th model out of 100\n","Fitting 8-th model out of 100\n","Fitting 9-th model out of 100\n","Fitting 10-th model out of 100\n","Fitting 11-th model out of 100\n","Fitting 12-th model out of 100\n","Fitting 13-th model out of 100\n","Fitting 14-th model out of 100\n","Fitting 15-th model out of 100\n","Fitting 16-th model out of 100\n","Fitting 17-th model out of 100\n","Fitting 18-th model out of 100\n","Fitting 19-th model out of 100\n","Fitting 20-th model out of 100\n","Fitting 21-th model out of 100\n","Fitting 22-th model out of 100\n","Fitting 23-th model out of 100\n","Fitting 24-th model out of 100\n","Fitting 25-th model out of 100\n","Fitting 26-th model out of 100\n","Fitting 27-th model out of 100\n","Fitting 28-th model out of 100\n","Fitting 29-th model out of 100\n","Fitting 30-th model out of 100\n","Fitting 31-th model out of 100\n","Fitting 32-th model out of 100\n","Fitting 33-th model out of 100\n","Fitting 34-th model out of 100\n","Fitting 35-th model out of 100\n","Fitting 36-th model out of 100\n","Fitting 37-th model out of 100\n","Fitting 38-th model out of 100\n","Fitting 39-th model out of 100\n","Fitting 40-th model out of 100\n","Fitting 41-th model out of 100\n","Fitting 42-th model out of 100\n","Fitting 43-th model out of 100\n","Fitting 44-th model out of 100\n","Fitting 45-th model out of 100\n","Fitting 46-th model out of 100\n","Fitting 47-th model out of 100\n","Fitting 48-th model out of 100\n","Fitting 49-th model out of 100\n","Fitting 50-th model out of 100\n","Fitting 51-th model out of 100\n","Fitting 52-th model out of 100\n","Fitting 53-th model out of 100\n","Fitting 54-th model out of 100\n","Fitting 55-th model out of 100\n","Fitting 56-th model out of 100\n","Fitting 57-th model out of 100\n","Fitting 58-th model out of 100\n","Fitting 59-th model out of 100\n","Fitting 60-th model out of 100\n","Fitting 61-th model out of 100\n","Fitting 62-th model out of 100\n","Fitting 63-th model out of 100\n","Fitting 64-th model out of 100\n","Fitting 65-th model out of 100\n","Fitting 66-th model out of 100\n","Fitting 67-th model out of 100\n","Fitting 68-th model out of 100\n","Fitting 69-th model out of 100\n","Fitting 70-th model out of 100\n","Fitting 71-th model out of 100\n","Fitting 72-th model out of 100\n","Fitting 73-th model out of 100\n","Fitting 74-th model out of 100\n","Fitting 75-th model out of 100\n","Fitting 76-th model out of 100\n","Fitting 77-th model out of 100\n","Fitting 78-th model out of 100\n","Fitting 79-th model out of 100\n","Fitting 80-th model out of 100\n","Fitting 81-th model out of 100\n","Fitting 82-th model out of 100\n","Fitting 83-th model out of 100\n","Fitting 84-th model out of 100\n","Fitting 85-th model out of 100\n","Fitting 86-th model out of 100\n","Fitting 87-th model out of 100\n","Fitting 88-th model out of 100\n","Fitting 89-th model out of 100\n","Fitting 90-th model out of 100\n","Fitting 91-th model out of 100\n","Fitting 92-th model out of 100\n","Fitting 93-th model out of 100\n","Fitting 94-th model out of 100\n","Fitting 95-th model out of 100\n","Fitting 96-th model out of 100\n","Fitting 97-th model out of 100\n","Fitting 98-th model out of 100\n","Fitting 99-th model out of 100\n","Fitting 100-th model out of 100\n","test my score forest: 0.839295484522789\n"]}]},{"cell_type":"markdown","source":["## Use Random Forest Regressor in housing data"],"metadata":{"id":"fozq-hwi9rvc"}},{"cell_type":"code","source":["ESTIMATORS = 100 \n","X_train, X_test, Y_train, Y_test = get_housing_data(test_size=0.3)\n","\n","baseline = LinearRegression()\n","single_tree = DecisionTreeRegressor()\n","sklearn_rf = sklearn.ensemble.RandomForestRegressor(n_estimators=ESTIMATORS)\n","my_rf_regressor = RandomForestRegressor(n_models=ESTIMATORS, n_features=5, trace_logs=False)\n","\n","single_tree.fit(X_train, Y_train)\n","baseline.fit(X_train, Y_train)\n","sklearn_rf.fit(X_train, Y_train)\n","my_rf_regressor.fit(X_train, Y_train)\n","\n","single_tree_scores = my_cross_val_score(single_tree, X_train, Y_train, cv=5, shuffle=True, random_state=123)\n","baseline_scores = my_cross_val_score(baseline, X_train, Y_train, cv=5, shuffle=True, random_state=123)\n","sklearn_rf_scores = my_cross_val_score(sklearn_rf, X_train, Y_train, cv=5, shuffle=True, random_state=123)\n","my_rf_regressor_scores = my_cross_val_score(my_rf_regressor, X_train, Y_train, cv=5, shuffle=True, random_state=123)\n","\n","print(\"test score single tree:\", single_tree.score(X_test, Y_test))\n","print(\"test score baseline:\", baseline.score(X_test, Y_test))\n","print(\"test sklearn score forest:\", sklearn_rf.score(X_test, Y_test))\n","print(\"test my score forest:\", my_rf_regressor.score(X_test, Y_test))\n","\n","\n","print(\"train score single tree:\", single_tree.score(X_train, Y_train))\n","print(\"train score baseline:\", baseline.score(X_train, Y_train))\n","print(\"train sklearn score forest:\", sklearn_rf.score(X_train, Y_train))\n","print(\"train my score forest:\", my_rf_regressor.score(X_train, Y_train))\n","\n","print(\"CV single tree:\", single_tree_scores.mean())\n","print(\"CV baseline:\", baseline_scores.mean())\n","print(\"CV sklearn forest:\", sklearn_rf_scores.mean())\n","print(\"CV sklearn forest:\", my_rf_regressor_scores.mean())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M948MrUaWiw1","executionInfo":{"status":"ok","timestamp":1644101306737,"user_tz":-60,"elapsed":329229,"user":{"displayName":"Marcin Figlewicz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641752571162350955"}},"outputId":"e24081c0-398d-4aa9-fa37-4e08641e6a99"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 1-th model out of 100\n","Fitting 2-th model out of 100\n","Fitting 3-th model out of 100\n","Fitting 4-th model out of 100\n","Fitting 5-th model out of 100\n","Fitting 6-th model out of 100\n","Fitting 7-th model out of 100\n","Fitting 8-th model out of 100\n","Fitting 9-th model out of 100\n","Fitting 10-th model out of 100\n","Fitting 11-th model out of 100\n","Fitting 12-th model out of 100\n","Fitting 13-th model out of 100\n","Fitting 14-th model out of 100\n","Fitting 15-th model out of 100\n","Fitting 16-th model out of 100\n","Fitting 17-th model out of 100\n","Fitting 18-th model out of 100\n","Fitting 19-th model out of 100\n","Fitting 20-th model out of 100\n","Fitting 21-th model out of 100\n","Fitting 22-th model out of 100\n","Fitting 23-th model out of 100\n","Fitting 24-th model out of 100\n","Fitting 25-th model out of 100\n","Fitting 26-th model out of 100\n","Fitting 27-th model out of 100\n","Fitting 28-th model out of 100\n","Fitting 29-th model out of 100\n","Fitting 30-th model out of 100\n","Fitting 31-th model out of 100\n","Fitting 32-th model out of 100\n","Fitting 33-th model out of 100\n","Fitting 34-th model out of 100\n","Fitting 35-th model out of 100\n","Fitting 36-th model out of 100\n","Fitting 37-th model out of 100\n","Fitting 38-th model out of 100\n","Fitting 39-th model out of 100\n","Fitting 40-th model out of 100\n","Fitting 41-th model out of 100\n","Fitting 42-th model out of 100\n","Fitting 43-th model out of 100\n","Fitting 44-th model out of 100\n","Fitting 45-th model out of 100\n","Fitting 46-th model out of 100\n","Fitting 47-th model out of 100\n","Fitting 48-th model out of 100\n","Fitting 49-th model out of 100\n","Fitting 50-th model out of 100\n","Fitting 51-th model out of 100\n","Fitting 52-th model out of 100\n","Fitting 53-th model out of 100\n","Fitting 54-th model out of 100\n","Fitting 55-th model out of 100\n","Fitting 56-th model out of 100\n","Fitting 57-th model out of 100\n","Fitting 58-th model out of 100\n","Fitting 59-th model out of 100\n","Fitting 60-th model out of 100\n","Fitting 61-th model out of 100\n","Fitting 62-th model out of 100\n","Fitting 63-th model out of 100\n","Fitting 64-th model out of 100\n","Fitting 65-th model out of 100\n","Fitting 66-th model out of 100\n","Fitting 67-th model out of 100\n","Fitting 68-th model out of 100\n","Fitting 69-th model out of 100\n","Fitting 70-th model out of 100\n","Fitting 71-th model out of 100\n","Fitting 72-th model out of 100\n","Fitting 73-th model out of 100\n","Fitting 74-th model out of 100\n","Fitting 75-th model out of 100\n","Fitting 76-th model out of 100\n","Fitting 77-th model out of 100\n","Fitting 78-th model out of 100\n","Fitting 79-th model out of 100\n","Fitting 80-th model out of 100\n","Fitting 81-th model out of 100\n","Fitting 82-th model out of 100\n","Fitting 83-th model out of 100\n","Fitting 84-th model out of 100\n","Fitting 85-th model out of 100\n","Fitting 86-th model out of 100\n","Fitting 87-th model out of 100\n","Fitting 88-th model out of 100\n","Fitting 89-th model out of 100\n","Fitting 90-th model out of 100\n","Fitting 91-th model out of 100\n","Fitting 92-th model out of 100\n","Fitting 93-th model out of 100\n","Fitting 94-th model out of 100\n","Fitting 95-th model out of 100\n","Fitting 96-th model out of 100\n","Fitting 97-th model out of 100\n","Fitting 98-th model out of 100\n","Fitting 99-th model out of 100\n","Fitting 100-th model out of 100\n","Fitting 1-th model out of 100\n","Fitting 2-th model out of 100\n","Fitting 3-th model out of 100\n","Fitting 4-th model out of 100\n","Fitting 5-th model out of 100\n","Fitting 6-th model out of 100\n","Fitting 7-th model out of 100\n","Fitting 8-th model out of 100\n","Fitting 9-th model out of 100\n","Fitting 10-th model out of 100\n","Fitting 11-th model out of 100\n","Fitting 12-th model out of 100\n","Fitting 13-th model out of 100\n","Fitting 14-th model out of 100\n","Fitting 15-th model out of 100\n","Fitting 16-th model out of 100\n","Fitting 17-th model out of 100\n","Fitting 18-th model out of 100\n","Fitting 19-th model out of 100\n","Fitting 20-th model out of 100\n","Fitting 21-th model out of 100\n","Fitting 22-th model out of 100\n","Fitting 23-th model out of 100\n","Fitting 24-th model out of 100\n","Fitting 25-th model out of 100\n","Fitting 26-th model out of 100\n","Fitting 27-th model out of 100\n","Fitting 28-th model out of 100\n","Fitting 29-th model out of 100\n","Fitting 30-th model out of 100\n","Fitting 31-th model out of 100\n","Fitting 32-th model out of 100\n","Fitting 33-th model out of 100\n","Fitting 34-th model out of 100\n","Fitting 35-th model out of 100\n","Fitting 36-th model out of 100\n","Fitting 37-th model out of 100\n","Fitting 38-th model out of 100\n","Fitting 39-th model out of 100\n","Fitting 40-th model out of 100\n","Fitting 41-th model out of 100\n","Fitting 42-th model out of 100\n","Fitting 43-th model out of 100\n","Fitting 44-th model out of 100\n","Fitting 45-th model out of 100\n","Fitting 46-th model out of 100\n","Fitting 47-th model out of 100\n","Fitting 48-th model out of 100\n","Fitting 49-th model out of 100\n","Fitting 50-th model out of 100\n","Fitting 51-th model out of 100\n","Fitting 52-th model out of 100\n","Fitting 53-th model out of 100\n","Fitting 54-th model out of 100\n","Fitting 55-th model out of 100\n","Fitting 56-th model out of 100\n","Fitting 57-th model out of 100\n","Fitting 58-th model out of 100\n","Fitting 59-th model out of 100\n","Fitting 60-th model out of 100\n","Fitting 61-th model out of 100\n","Fitting 62-th model out of 100\n","Fitting 63-th model out of 100\n","Fitting 64-th model out of 100\n","Fitting 65-th model out of 100\n","Fitting 66-th model out of 100\n","Fitting 67-th model out of 100\n","Fitting 68-th model out of 100\n","Fitting 69-th model out of 100\n","Fitting 70-th model out of 100\n","Fitting 71-th model out of 100\n","Fitting 72-th model out of 100\n","Fitting 73-th model out of 100\n","Fitting 74-th model out of 100\n","Fitting 75-th model out of 100\n","Fitting 76-th model out of 100\n","Fitting 77-th model out of 100\n","Fitting 78-th model out of 100\n","Fitting 79-th model out of 100\n","Fitting 80-th model out of 100\n","Fitting 81-th model out of 100\n","Fitting 82-th model out of 100\n","Fitting 83-th model out of 100\n","Fitting 84-th model out of 100\n","Fitting 85-th model out of 100\n","Fitting 86-th model out of 100\n","Fitting 87-th model out of 100\n","Fitting 88-th model out of 100\n","Fitting 89-th model out of 100\n","Fitting 90-th model out of 100\n","Fitting 91-th model out of 100\n","Fitting 92-th model out of 100\n","Fitting 93-th model out of 100\n","Fitting 94-th model out of 100\n","Fitting 95-th model out of 100\n","Fitting 96-th model out of 100\n","Fitting 97-th model out of 100\n","Fitting 98-th model out of 100\n","Fitting 99-th model out of 100\n","Fitting 100-th model out of 100\n","Fitting 1-th model out of 100\n","Fitting 2-th model out of 100\n","Fitting 3-th model out of 100\n","Fitting 4-th model out of 100\n","Fitting 5-th model out of 100\n","Fitting 6-th model out of 100\n","Fitting 7-th model out of 100\n","Fitting 8-th model out of 100\n","Fitting 9-th model out of 100\n","Fitting 10-th model out of 100\n","Fitting 11-th model out of 100\n","Fitting 12-th model out of 100\n","Fitting 13-th model out of 100\n","Fitting 14-th model out of 100\n","Fitting 15-th model out of 100\n","Fitting 16-th model out of 100\n","Fitting 17-th model out of 100\n","Fitting 18-th model out of 100\n","Fitting 19-th model out of 100\n","Fitting 20-th model out of 100\n","Fitting 21-th model out of 100\n","Fitting 22-th model out of 100\n","Fitting 23-th model out of 100\n","Fitting 24-th model out of 100\n","Fitting 25-th model out of 100\n","Fitting 26-th model out of 100\n","Fitting 27-th model out of 100\n","Fitting 28-th model out of 100\n","Fitting 29-th model out of 100\n","Fitting 30-th model out of 100\n","Fitting 31-th model out of 100\n","Fitting 32-th model out of 100\n","Fitting 33-th model out of 100\n","Fitting 34-th model out of 100\n","Fitting 35-th model out of 100\n","Fitting 36-th model out of 100\n","Fitting 37-th model out of 100\n","Fitting 38-th model out of 100\n","Fitting 39-th model out of 100\n","Fitting 40-th model out of 100\n","Fitting 41-th model out of 100\n","Fitting 42-th model out of 100\n","Fitting 43-th model out of 100\n","Fitting 44-th model out of 100\n","Fitting 45-th model out of 100\n","Fitting 46-th model out of 100\n","Fitting 47-th model out of 100\n","Fitting 48-th model out of 100\n","Fitting 49-th model out of 100\n","Fitting 50-th model out of 100\n","Fitting 51-th model out of 100\n","Fitting 52-th model out of 100\n","Fitting 53-th model out of 100\n","Fitting 54-th model out of 100\n","Fitting 55-th model out of 100\n","Fitting 56-th model out of 100\n","Fitting 57-th model out of 100\n","Fitting 58-th model out of 100\n","Fitting 59-th model out of 100\n","Fitting 60-th model out of 100\n","Fitting 61-th model out of 100\n","Fitting 62-th model out of 100\n","Fitting 63-th model out of 100\n","Fitting 64-th model out of 100\n","Fitting 65-th model out of 100\n","Fitting 66-th model out of 100\n","Fitting 67-th model out of 100\n","Fitting 68-th model out of 100\n","Fitting 69-th model out of 100\n","Fitting 70-th model out of 100\n","Fitting 71-th model out of 100\n","Fitting 72-th model out of 100\n","Fitting 73-th model out of 100\n","Fitting 74-th model out of 100\n","Fitting 75-th model out of 100\n","Fitting 76-th model out of 100\n","Fitting 77-th model out of 100\n","Fitting 78-th model out of 100\n","Fitting 79-th model out of 100\n","Fitting 80-th model out of 100\n","Fitting 81-th model out of 100\n","Fitting 82-th model out of 100\n","Fitting 83-th model out of 100\n","Fitting 84-th model out of 100\n","Fitting 85-th model out of 100\n","Fitting 86-th model out of 100\n","Fitting 87-th model out of 100\n","Fitting 88-th model out of 100\n","Fitting 89-th model out of 100\n","Fitting 90-th model out of 100\n","Fitting 91-th model out of 100\n","Fitting 92-th model out of 100\n","Fitting 93-th model out of 100\n","Fitting 94-th model out of 100\n","Fitting 95-th model out of 100\n","Fitting 96-th model out of 100\n","Fitting 97-th model out of 100\n","Fitting 98-th model out of 100\n","Fitting 99-th model out of 100\n","Fitting 100-th model out of 100\n","Fitting 1-th model out of 100\n","Fitting 2-th model out of 100\n","Fitting 3-th model out of 100\n","Fitting 4-th model out of 100\n","Fitting 5-th model out of 100\n","Fitting 6-th model out of 100\n","Fitting 7-th model out of 100\n","Fitting 8-th model out of 100\n","Fitting 9-th model out of 100\n","Fitting 10-th model out of 100\n","Fitting 11-th model out of 100\n","Fitting 12-th model out of 100\n","Fitting 13-th model out of 100\n","Fitting 14-th model out of 100\n","Fitting 15-th model out of 100\n","Fitting 16-th model out of 100\n","Fitting 17-th model out of 100\n","Fitting 18-th model out of 100\n","Fitting 19-th model out of 100\n","Fitting 20-th model out of 100\n","Fitting 21-th model out of 100\n","Fitting 22-th model out of 100\n","Fitting 23-th model out of 100\n","Fitting 24-th model out of 100\n","Fitting 25-th model out of 100\n","Fitting 26-th model out of 100\n","Fitting 27-th model out of 100\n","Fitting 28-th model out of 100\n","Fitting 29-th model out of 100\n","Fitting 30-th model out of 100\n","Fitting 31-th model out of 100\n","Fitting 32-th model out of 100\n","Fitting 33-th model out of 100\n","Fitting 34-th model out of 100\n","Fitting 35-th model out of 100\n","Fitting 36-th model out of 100\n","Fitting 37-th model out of 100\n","Fitting 38-th model out of 100\n","Fitting 39-th model out of 100\n","Fitting 40-th model out of 100\n","Fitting 41-th model out of 100\n","Fitting 42-th model out of 100\n","Fitting 43-th model out of 100\n","Fitting 44-th model out of 100\n","Fitting 45-th model out of 100\n","Fitting 46-th model out of 100\n","Fitting 47-th model out of 100\n","Fitting 48-th model out of 100\n","Fitting 49-th model out of 100\n","Fitting 50-th model out of 100\n","Fitting 51-th model out of 100\n","Fitting 52-th model out of 100\n","Fitting 53-th model out of 100\n","Fitting 54-th model out of 100\n","Fitting 55-th model out of 100\n","Fitting 56-th model out of 100\n","Fitting 57-th model out of 100\n","Fitting 58-th model out of 100\n","Fitting 59-th model out of 100\n","Fitting 60-th model out of 100\n","Fitting 61-th model out of 100\n","Fitting 62-th model out of 100\n","Fitting 63-th model out of 100\n","Fitting 64-th model out of 100\n","Fitting 65-th model out of 100\n","Fitting 66-th model out of 100\n","Fitting 67-th model out of 100\n","Fitting 68-th model out of 100\n","Fitting 69-th model out of 100\n","Fitting 70-th model out of 100\n","Fitting 71-th model out of 100\n","Fitting 72-th model out of 100\n","Fitting 73-th model out of 100\n","Fitting 74-th model out of 100\n","Fitting 75-th model out of 100\n","Fitting 76-th model out of 100\n","Fitting 77-th model out of 100\n","Fitting 78-th model out of 100\n","Fitting 79-th model out of 100\n","Fitting 80-th model out of 100\n","Fitting 81-th model out of 100\n","Fitting 82-th model out of 100\n","Fitting 83-th model out of 100\n","Fitting 84-th model out of 100\n","Fitting 85-th model out of 100\n","Fitting 86-th model out of 100\n","Fitting 87-th model out of 100\n","Fitting 88-th model out of 100\n","Fitting 89-th model out of 100\n","Fitting 90-th model out of 100\n","Fitting 91-th model out of 100\n","Fitting 92-th model out of 100\n","Fitting 93-th model out of 100\n","Fitting 94-th model out of 100\n","Fitting 95-th model out of 100\n","Fitting 96-th model out of 100\n","Fitting 97-th model out of 100\n","Fitting 98-th model out of 100\n","Fitting 99-th model out of 100\n","Fitting 100-th model out of 100\n","Fitting 1-th model out of 100\n","Fitting 2-th model out of 100\n","Fitting 3-th model out of 100\n","Fitting 4-th model out of 100\n","Fitting 5-th model out of 100\n","Fitting 6-th model out of 100\n","Fitting 7-th model out of 100\n","Fitting 8-th model out of 100\n","Fitting 9-th model out of 100\n","Fitting 10-th model out of 100\n","Fitting 11-th model out of 100\n","Fitting 12-th model out of 100\n","Fitting 13-th model out of 100\n","Fitting 14-th model out of 100\n","Fitting 15-th model out of 100\n","Fitting 16-th model out of 100\n","Fitting 17-th model out of 100\n","Fitting 18-th model out of 100\n","Fitting 19-th model out of 100\n","Fitting 20-th model out of 100\n","Fitting 21-th model out of 100\n","Fitting 22-th model out of 100\n","Fitting 23-th model out of 100\n","Fitting 24-th model out of 100\n","Fitting 25-th model out of 100\n","Fitting 26-th model out of 100\n","Fitting 27-th model out of 100\n","Fitting 28-th model out of 100\n","Fitting 29-th model out of 100\n","Fitting 30-th model out of 100\n","Fitting 31-th model out of 100\n","Fitting 32-th model out of 100\n","Fitting 33-th model out of 100\n","Fitting 34-th model out of 100\n","Fitting 35-th model out of 100\n","Fitting 36-th model out of 100\n","Fitting 37-th model out of 100\n","Fitting 38-th model out of 100\n","Fitting 39-th model out of 100\n","Fitting 40-th model out of 100\n","Fitting 41-th model out of 100\n","Fitting 42-th model out of 100\n","Fitting 43-th model out of 100\n","Fitting 44-th model out of 100\n","Fitting 45-th model out of 100\n","Fitting 46-th model out of 100\n","Fitting 47-th model out of 100\n","Fitting 48-th model out of 100\n","Fitting 49-th model out of 100\n","Fitting 50-th model out of 100\n","Fitting 51-th model out of 100\n","Fitting 52-th model out of 100\n","Fitting 53-th model out of 100\n","Fitting 54-th model out of 100\n","Fitting 55-th model out of 100\n","Fitting 56-th model out of 100\n","Fitting 57-th model out of 100\n","Fitting 58-th model out of 100\n","Fitting 59-th model out of 100\n","Fitting 60-th model out of 100\n","Fitting 61-th model out of 100\n","Fitting 62-th model out of 100\n","Fitting 63-th model out of 100\n","Fitting 64-th model out of 100\n","Fitting 65-th model out of 100\n","Fitting 66-th model out of 100\n","Fitting 67-th model out of 100\n","Fitting 68-th model out of 100\n","Fitting 69-th model out of 100\n","Fitting 70-th model out of 100\n","Fitting 71-th model out of 100\n","Fitting 72-th model out of 100\n","Fitting 73-th model out of 100\n","Fitting 74-th model out of 100\n","Fitting 75-th model out of 100\n","Fitting 76-th model out of 100\n","Fitting 77-th model out of 100\n","Fitting 78-th model out of 100\n","Fitting 79-th model out of 100\n","Fitting 80-th model out of 100\n","Fitting 81-th model out of 100\n","Fitting 82-th model out of 100\n","Fitting 83-th model out of 100\n","Fitting 84-th model out of 100\n","Fitting 85-th model out of 100\n","Fitting 86-th model out of 100\n","Fitting 87-th model out of 100\n","Fitting 88-th model out of 100\n","Fitting 89-th model out of 100\n","Fitting 90-th model out of 100\n","Fitting 91-th model out of 100\n","Fitting 92-th model out of 100\n","Fitting 93-th model out of 100\n","Fitting 94-th model out of 100\n","Fitting 95-th model out of 100\n","Fitting 96-th model out of 100\n","Fitting 97-th model out of 100\n","Fitting 98-th model out of 100\n","Fitting 99-th model out of 100\n","Fitting 100-th model out of 100\n","Fitting 1-th model out of 100\n","Fitting 2-th model out of 100\n","Fitting 3-th model out of 100\n","Fitting 4-th model out of 100\n","Fitting 5-th model out of 100\n","Fitting 6-th model out of 100\n","Fitting 7-th model out of 100\n","Fitting 8-th model out of 100\n","Fitting 9-th model out of 100\n","Fitting 10-th model out of 100\n","Fitting 11-th model out of 100\n","Fitting 12-th model out of 100\n","Fitting 13-th model out of 100\n","Fitting 14-th model out of 100\n","Fitting 15-th model out of 100\n","Fitting 16-th model out of 100\n","Fitting 17-th model out of 100\n","Fitting 18-th model out of 100\n","Fitting 19-th model out of 100\n","Fitting 20-th model out of 100\n","Fitting 21-th model out of 100\n","Fitting 22-th model out of 100\n","Fitting 23-th model out of 100\n","Fitting 24-th model out of 100\n","Fitting 25-th model out of 100\n","Fitting 26-th model out of 100\n","Fitting 27-th model out of 100\n","Fitting 28-th model out of 100\n","Fitting 29-th model out of 100\n","Fitting 30-th model out of 100\n","Fitting 31-th model out of 100\n","Fitting 32-th model out of 100\n","Fitting 33-th model out of 100\n","Fitting 34-th model out of 100\n","Fitting 35-th model out of 100\n","Fitting 36-th model out of 100\n","Fitting 37-th model out of 100\n","Fitting 38-th model out of 100\n","Fitting 39-th model out of 100\n","Fitting 40-th model out of 100\n","Fitting 41-th model out of 100\n","Fitting 42-th model out of 100\n","Fitting 43-th model out of 100\n","Fitting 44-th model out of 100\n","Fitting 45-th model out of 100\n","Fitting 46-th model out of 100\n","Fitting 47-th model out of 100\n","Fitting 48-th model out of 100\n","Fitting 49-th model out of 100\n","Fitting 50-th model out of 100\n","Fitting 51-th model out of 100\n","Fitting 52-th model out of 100\n","Fitting 53-th model out of 100\n","Fitting 54-th model out of 100\n","Fitting 55-th model out of 100\n","Fitting 56-th model out of 100\n","Fitting 57-th model out of 100\n","Fitting 58-th model out of 100\n","Fitting 59-th model out of 100\n","Fitting 60-th model out of 100\n","Fitting 61-th model out of 100\n","Fitting 62-th model out of 100\n","Fitting 63-th model out of 100\n","Fitting 64-th model out of 100\n","Fitting 65-th model out of 100\n","Fitting 66-th model out of 100\n","Fitting 67-th model out of 100\n","Fitting 68-th model out of 100\n","Fitting 69-th model out of 100\n","Fitting 70-th model out of 100\n","Fitting 71-th model out of 100\n","Fitting 72-th model out of 100\n","Fitting 73-th model out of 100\n","Fitting 74-th model out of 100\n","Fitting 75-th model out of 100\n","Fitting 76-th model out of 100\n","Fitting 77-th model out of 100\n","Fitting 78-th model out of 100\n","Fitting 79-th model out of 100\n","Fitting 80-th model out of 100\n","Fitting 81-th model out of 100\n","Fitting 82-th model out of 100\n","Fitting 83-th model out of 100\n","Fitting 84-th model out of 100\n","Fitting 85-th model out of 100\n","Fitting 86-th model out of 100\n","Fitting 87-th model out of 100\n","Fitting 88-th model out of 100\n","Fitting 89-th model out of 100\n","Fitting 90-th model out of 100\n","Fitting 91-th model out of 100\n","Fitting 92-th model out of 100\n","Fitting 93-th model out of 100\n","Fitting 94-th model out of 100\n","Fitting 95-th model out of 100\n","Fitting 96-th model out of 100\n","Fitting 97-th model out of 100\n","Fitting 98-th model out of 100\n","Fitting 99-th model out of 100\n","Fitting 100-th model out of 100\n","test score single tree: 0.6700606457936384\n","test score baseline: 0.7044962214730194\n","test sklearn score forest: 0.8575018129981615\n","test my score forest: 0.7974244328423132\n","train score single tree: 0.8589507436107016\n","train score baseline: 0.74131414118766\n","train sklearn score forest: 0.9436154028262965\n","train my score forest: 0.9489084739091208\n","CV single tree: 0.6215639977637244\n","CV baseline: 0.6975655383424016\n","CV sklearn forest: 0.8292896223251505\n","CV sklearn forest: 0.7872889651364605\n"]}]},{"cell_type":"markdown","source":["## Use Random Forest Classifier in mushroom data"],"metadata":{"id":"XvDpVZG-oJus"}},{"cell_type":"code","source":["ESTIMATORS = 10\n","X_train, X_test, Y_train, Y_test = get_mushroom_data(test_size=0.3)\n","\n","baseline = LogisticRegression()\n","single_tree = DecisionTreeClassifier()\n","sklearn_rf = sklearn.ensemble.RandomForestClassifier(n_estimators=ESTIMATORS)\n","my_rf_classifier = RandomForestRegressor(n_models=ESTIMATORS, n_features=int(np.sqrt(X_train.shape[1])), trace_logs=False)\n","\n","single_tree.fit(X_train, Y_train)\n","baseline.fit(X_train, Y_train)\n","sklearn_rf.fit(X_train, Y_train)\n","my_rf_classifier.fit(X_train, Y_train)\n","\n","single_tree_scores = my_cross_val_score(single_tree, X_train, Y_train, cv=5, shuffle=True, random_state=123)\n","baseline_scores = my_cross_val_score(baseline, X_train, Y_train, cv=5, shuffle=True, random_state=123)\n","sklearn_rf_scores = my_cross_val_score(sklearn_rf, X_train, Y_train, cv=5, shuffle=True, random_state=123)\n","my_rf_classifier_scores = my_cross_val_score(my_rf_classifier, X_train, Y_train, cv=5, shuffle=True, random_state=123)\n","\n","print(\"test score single tree:\", single_tree.score(X_test, Y_test))\n","print(\"test score baseline:\", baseline.score(X_test, Y_test))\n","print(\"test sklearn score forest:\", sklearn_rf.score(X_test, Y_test))\n","print(\"test my score forest:\", my_rf_classifier.score(X_test, Y_test))\n","\n","\n","print(\"train score single tree:\", single_tree.score(X_train, Y_train))\n","print(\"train score baseline:\", baseline.score(X_train, Y_train))\n","print(\"train sklearn score forest:\", sklearn_rf.score(X_train, Y_train))\n","print(\"train my score forest:\", my_rf_classifier.score(X_train, Y_train))\n","\n","print(\"CV single tree:\", single_tree_scores.mean())\n","print(\"CV baseline:\", baseline_scores.mean())\n","print(\"CV sklearn forest:\", sklearn_rf_scores.mean())\n","print(\"CV sklearn forest:\", my_rf_classifier_scores.mean())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLYA_oRTS6ep","executionInfo":{"status":"ok","timestamp":1644101394871,"user_tz":-60,"elapsed":88154,"user":{"displayName":"Marcin Figlewicz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641752571162350955"}},"outputId":"887051a2-c24b-4d20-895c-97a4ecd55a3c"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["dimensionality: 139\n","Fitting 1-th model out of 10\n","Fitting 2-th model out of 10\n","Fitting 3-th model out of 10\n","Fitting 4-th model out of 10\n","Fitting 5-th model out of 10\n","Fitting 6-th model out of 10\n","Fitting 7-th model out of 10\n","Fitting 8-th model out of 10\n","Fitting 9-th model out of 10\n","Fitting 10-th model out of 10\n","Fitting 1-th model out of 10\n","Fitting 2-th model out of 10\n","Fitting 3-th model out of 10\n","Fitting 4-th model out of 10\n","Fitting 5-th model out of 10\n","Fitting 6-th model out of 10\n","Fitting 7-th model out of 10\n","Fitting 8-th model out of 10\n","Fitting 9-th model out of 10\n","Fitting 10-th model out of 10\n","Fitting 1-th model out of 10\n","Fitting 2-th model out of 10\n","Fitting 3-th model out of 10\n","Fitting 4-th model out of 10\n","Fitting 5-th model out of 10\n","Fitting 6-th model out of 10\n","Fitting 7-th model out of 10\n","Fitting 8-th model out of 10\n","Fitting 9-th model out of 10\n","Fitting 10-th model out of 10\n","Fitting 1-th model out of 10\n","Fitting 2-th model out of 10\n","Fitting 3-th model out of 10\n","Fitting 4-th model out of 10\n","Fitting 5-th model out of 10\n","Fitting 6-th model out of 10\n","Fitting 7-th model out of 10\n","Fitting 8-th model out of 10\n","Fitting 9-th model out of 10\n","Fitting 10-th model out of 10\n","Fitting 1-th model out of 10\n","Fitting 2-th model out of 10\n","Fitting 3-th model out of 10\n","Fitting 4-th model out of 10\n","Fitting 5-th model out of 10\n","Fitting 6-th model out of 10\n","Fitting 7-th model out of 10\n","Fitting 8-th model out of 10\n","Fitting 9-th model out of 10\n","Fitting 10-th model out of 10\n","Fitting 1-th model out of 10\n","Fitting 2-th model out of 10\n","Fitting 3-th model out of 10\n","Fitting 4-th model out of 10\n","Fitting 5-th model out of 10\n","Fitting 6-th model out of 10\n","Fitting 7-th model out of 10\n","Fitting 8-th model out of 10\n","Fitting 9-th model out of 10\n","Fitting 10-th model out of 10\n","test score single tree: 1.0\n","test score baseline: 0.9995898277276456\n","test sklearn score forest: 1.0\n","test my score forest: 0.9881180051337286\n","train score single tree: 1.0\n","train score baseline: 1.0\n","train sklearn score forest: 1.0\n","train my score forest: 0.9895205880093983\n","CV single tree: 0.9996485061511423\n","CV baseline: 0.9994727592267136\n","CV sklearn forest: 0.9998242530755711\n","CV sklearn forest: 0.9902783533603351\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"8dmLYpsrTdSG","executionInfo":{"status":"ok","timestamp":1644101394872,"user_tz":-60,"elapsed":23,"user":{"displayName":"Marcin Figlewicz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11641752571162350955"}}},"execution_count":26,"outputs":[]}]}